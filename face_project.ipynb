{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNq4czWGBA/0/2kCBwPu2A2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vangogh1803/face_rec/blob/main/face_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGUpI73NNboF",
        "outputId": "39df4789-5f48-4b9f-87d9-7b919b6a155a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "BASE_DIR = \"/content/drive/MyDrive/face_proj\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "dZUBkHy2Q9ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1wBrxT9Rew7",
        "outputId": "55df82b4-643c-4dad-e0b7-beacc0e1fa64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets\n"
      ],
      "metadata": {
        "id": "9awkkYh-Xet7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "3vOLWV_Edzq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets list | head\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ftt1gIhFW4VA",
        "outputId": "cc2afc4a-fd99-4466-e7bc-da341e306a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                  title                                           size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "---------------------------------------------------  ----------------------------------------  ----------  --------------------------  -------------  ---------  ---------------  \n",
            "neurocipher/heartdisease                             Heart Disease                                   3491  2025-12-11 15:29:14.327000           2114        204  1.0              \n",
            "ahmeduzaki/wind-and-solar-energy-production-dataset  Wind & Solar Energy Production Dataset        395372  2026-01-02 21:06:22.780000              0         35  1.0              \n",
            "kundanbedmutha/exam-score-prediction-dataset         Exam Score Prediction Dataset                 325454  2025-11-28 07:29:01.047000           5863        249  1.0              \n",
            "guriya79/heart-failure-prediction-dataset            Heart Failure Clinical Records Study            4067  2025-12-27 05:13:37.790000              0         28  1.0              \n",
            "neurocipher/student-performance                      Student Performance                            49705  2025-12-12 12:06:28.973000           1261        120  1.0              \n",
            "ishank2005/salary-csv                                Salary.csv                                       392  2025-12-29 15:48:58.240000              0         27  1.0              \n",
            "dansbecker/melbourne-housing-snapshot                Melbourne Housing Snapshot                    461423  2018-06-05 12:52:24.087000         200095       1722  0.7058824        \n",
            "datasnaek/youtube-new                                Trending YouTube Video Statistics          210575746  2019-06-03 00:56:47.177000         292850       5843  0.7941176        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d kenny3s/casia-webface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuErHUOMECec",
        "outputId": "610ccdc3-fcbc-460a-8f9b-15c17f52a64e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/kenny3s/casia-webface\n",
            "License(s): MIT\n",
            "casia-webface.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d jessicali9530/lfw-dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJMsw19Aejo8",
        "outputId": "d0501267-dd96-49cf-ee27-15d8092221dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/jessicali9530/lfw-dataset\n",
            "License(s): other\n",
            "Downloading lfw-dataset.zip to /content\n",
            "  0% 0.00/112M [00:00<?, ?B/s]\n",
            "100% 112M/112M [00:00<00:00, 1.31GB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Jw2gyorigoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/data/lfw-dataset.zip -d /content/data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osuXbaXLezNw",
        "outputId": "f7a887e3-5cb1-4556-f22b-93cd0c99ff44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/data/lfw-dataset.zip, /content/data/lfw-dataset.zip.zip or /content/data/lfw-dataset.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/casia-webface.zip -d /content/data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUeceF4JFlop",
        "outputId": "5cce6eb2-b826-40ae-a61c-eeb2c4f0fa16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/casia-webface.zip\n",
            "replace /content/data/datasets/0000045/001.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "one\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "# Build identity â†’ image indices map\n",
        "id_to_indices = defaultdict(list)\n",
        "for idx, lbl in enumerate(dataset.targets):\n",
        "    id_to_indices[lbl].append(idx)\n",
        "\n",
        "# Keep identities with >= 2 images\n",
        "valid_ids = [lbl for lbl, idxs in id_to_indices.items() if len(idxs) >= 2]\n",
        "print(\"Valid identities:\", len(valid_ids))\n",
        "\n",
        "# Split identities\n",
        "random.shuffle(valid_ids)\n",
        "split = int(0.8 * len(valid_ids))\n",
        "\n",
        "train_ids = valid_ids[:split]\n",
        "val_ids   = valid_ids[split:]\n",
        "\n",
        "# Build index lists\n",
        "train_idx, val_idx = [], []\n",
        "\n",
        "for lbl in train_ids:\n",
        "    train_idx.extend(id_to_indices[lbl])\n",
        "\n",
        "for lbl in val_ids:\n",
        "    val_idx.extend(id_to_indices[lbl])\n",
        "\n",
        "print(\"Train images:\", len(train_idx))\n",
        "print(\"Val images:\", len(val_idx))\n",
        "\n",
        "# Build datasets\n",
        "train_dataset = Subset(dataset, train_idx)\n",
        "val_dataset   = Subset(dataset, val_idx)\n"
      ],
      "metadata": {
        "id": "QQTozuQLQxWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader=DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader=DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "1rV3adeYRBFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install facenet-pytorch\n"
      ],
      "metadata": {
        "id": "evX_LhaymGMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from facenet_pytorch import MTCNN\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "mtcnn = MTCNN(\n",
        "    image_size=224,\n",
        "    margin=20,\n",
        "    keep_all=False,\n",
        "    device=device\n",
        ")\n"
      ],
      "metadata": {
        "id": "rjnKaWcCnqDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class FaceDetectDataset(Dataset):\n",
        "    def __init__(self, base_dataset, mtcnn):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.mtcnn = mtcnn\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.base_dataset[idx]\n",
        "\n",
        "        # ensure PIL image\n",
        "        if isinstance(img, torch.Tensor):\n",
        "            img = transforms.ToPILImage()(img)\n",
        "\n",
        "        # detect face\n",
        "        face = self.mtcnn(img)\n",
        "\n",
        "        if face is None:\n",
        "            # fallback: resize original image\n",
        "            face = transforms.Resize((224,224))(img)\n",
        "            face = transforms.ToTensor()(face)\n",
        "        else:\n",
        "            # face is already tensor\n",
        "            face = face\n",
        "\n",
        "        return face, label\n"
      ],
      "metadata": {
        "id": "4QWqHLiCn8AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Classification*"
      ],
      "metadata": {
        "id": "5B-6YrhPYonw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),\n",
        "#     transforms.Grayscale(num_output_channels=3),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(\n",
        "#         mean=[0.485, 0.456, 0.406],\n",
        "#         std=[0.229, 0.224, 0.225]\n",
        "#     )\n",
        "# ])\n",
        "##augmentation\n",
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomApply([\n",
        "        transforms.ColorJitter(\n",
        "            brightness=0.2,\n",
        "            contrast=0.2,\n",
        "            saturation=0.2\n",
        "        )\n",
        "    ], p=0.5),\n",
        "    transforms.RandomGrayscale(p=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n"
      ],
      "metadata": {
        "id": "Zk7h0UqVXm2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/data -maxdepth 3 -type d | head -n 20\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATC4NalhH1c_",
        "outputId": "2ae42ef1-a74c-463c-f975-aa8132ffb953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n",
            "/content/data/casia_dev\n",
            "/content/data/casia_dev/0000696\n",
            "/content/data/casia_dev/0000981\n",
            "/content/data/casia_dev/0000422\n",
            "/content/data/casia_dev/0000948\n",
            "/content/data/casia_dev/0000675\n",
            "/content/data/casia_dev/0000260\n",
            "/content/data/casia_dev/0000996\n",
            "/content/data/casia_dev/0000304\n",
            "/content/data/casia_dev/0000880\n",
            "/content/data/casia_dev/0000570\n",
            "/content/data/casia_dev/0000275\n",
            "/content/data/casia_dev/0000670\n",
            "/content/data/casia_dev/0000616\n",
            "/content/data/casia_dev/0000615\n",
            "/content/data/casia_dev/0000133\n",
            "/content/data/casia_dev/0000394\n",
            "/content/data/casia_dev/0000836\n",
            "/content/data/casia_dev/0000373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/data/datasets\"\n",
        "\n",
        "dataset = datasets.ImageFolder(\n",
        "    root=dataset_path,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "print(\"Total images:\", len(dataset))\n",
        "print(\"Total identities:\", len(dataset.classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vby43vWPZFLA",
        "outputId": "9e239b6f-f6b6-4ae5-e422-2c8adea161a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 452960\n",
            "Total identities: 10575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEV_IDENTITIES = 300\n",
        "IMAGES_PER_ID = 25\n",
        "import os, shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "SRC = \"/content/data/datasets\"\n",
        "DST = \"/content/data/casia_dev\"\n",
        "\n",
        "os.makedirs(DST, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "QDXPYc61RdLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "identities = sorted(os.listdir(SRC))[:DEV_IDENTITIES]\n",
        "\n",
        "for ident in identities:\n",
        "    src_id = os.path.join(SRC, ident)\n",
        "    dst_id = os.path.join(DST, ident)\n",
        "    os.makedirs(dst_id, exist_ok=True)\n",
        "\n",
        "    imgs = os.listdir(src_id)[:IMAGES_PER_ID]\n",
        "    for img in imgs:\n",
        "        shutil.copy(\n",
        "            os.path.join(src_id, img),\n",
        "            os.path.join(dst_id, img)\n",
        "        )\n",
        "\n",
        "print(\"CASIA dev subset ready\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Fm8mTUvRp0o",
        "outputId": "90475997-f1d4-44dc-87d9-6e0759dc6725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CASIA dev subset ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/data/casia_dev\"\n",
        "\n",
        "dataset = datasets.ImageFolder(\n",
        "    root=dataset_path,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "print(\"Images:\", len(dataset))\n",
        "print(\"Identities:\", len(dataset.classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0jvQ0wbRuiL",
        "outputId": "ca0fdd40-525b-4969-d98e-4829eac49123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images: 7156\n",
            "Identities: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size=int(0.8*len(dataset))\n",
        "val_size=len(dataset)-train_size\n",
        "train_dataset, val_dataset=torch.utils.data.random_split(\n",
        "    dataset, [train_size, val_size]\n",
        "  )\n"
      ],
      "metadata": {
        "id": "yfftQrDQaHR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Subset\n",
        "import random\n",
        "\n",
        "labels = dataset.targets\n",
        "id_to_idx = {}\n",
        "\n",
        "for i, lbl in enumerate(labels):\n",
        "    id_to_idx.setdefault(lbl, []).append(i)\n",
        "\n",
        "ids = list(id_to_idx.keys())\n",
        "random.shuffle(ids)\n",
        "\n",
        "split = int(0.8 * len(ids))\n",
        "train_ids = ids[:split]\n",
        "val_ids = ids[split:]\n",
        "\n",
        "train_idx, val_idx = [], []\n",
        "\n",
        "for i in train_ids:\n",
        "    train_idx.extend(id_to_idx[i])\n",
        "\n",
        "for i in val_ids:\n",
        "    val_idx.extend(id_to_idx[i])\n",
        "\n",
        "train_dataset = Subset(dataset, train_idx)\n",
        "val_dataset = Subset(dataset, val_idx)\n"
      ],
      "metadata": {
        "id": "JAbnvfTVSPbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "5Uwp6EltTa4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=models.resnet18(pretrained=True)\n",
        "model.fc=nn.Linear(model.fc.in_features, len(dataset.classes))\n",
        "model=model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kp11mrF4gMzp",
        "outputId": "095b12d5-7f5d-4aed-cdb8-57dea90bc5b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "aLZK0WB9g3E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train now:\n",
        "Epochs=3\n",
        "for epoch in range(Epochs):\n",
        "  model.train()\n",
        "  running_loss=0\n",
        "  correct=0\n",
        "  total=0\n",
        "\n",
        "  for images, labels in train_loader:\n",
        "    images=images.to(device)\n",
        "    labels=labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output=model(images)\n",
        "    loss=criterion(output, labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss+=loss.item()\n",
        "    _, preds=torch.max(output, 1)\n",
        "    correct+=(preds==labels).sum().item()\n",
        "    total+=labels.size(0)\n",
        "\n",
        "    train_acc=100*correct/total\n",
        "    print(f\"Epoch [{epoch+1}/{Epochs}] \"\n",
        "          f\"Loss: {running_loss:.3f} | Train Acc: {train_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsqbRrLVhF2Z",
        "outputId": "182feee5-9fe6-47bc-e088-3e5b27976277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3] Loss: 5.936 | Train Acc: 0.00%\n",
            "Epoch [1/3] Loss: 11.948 | Train Acc: 0.00%\n",
            "Epoch [1/3] Loss: 18.245 | Train Acc: 0.00%\n",
            "Epoch [1/3] Loss: 24.266 | Train Acc: 0.00%\n",
            "Epoch [1/3] Loss: 30.409 | Train Acc: 0.00%\n",
            "Epoch [1/3] Loss: 36.517 | Train Acc: 0.00%\n",
            "Epoch [1/3] Loss: 42.386 | Train Acc: 0.00%\n",
            "Epoch [1/3] Loss: 48.513 | Train Acc: 0.00%\n",
            "Epoch [1/3] Loss: 54.614 | Train Acc: 0.00%\n",
            "Epoch [1/3] Loss: 60.627 | Train Acc: 0.00%\n",
            "Epoch [1/3] Loss: 66.614 | Train Acc: 0.00%\n",
            "Epoch [1/3] Loss: 72.790 | Train Acc: 0.00%\n",
            "Epoch [1/3] Loss: 78.918 | Train Acc: 0.00%\n",
            "Epoch [1/3] Loss: 85.029 | Train Acc: 0.00%\n",
            "Epoch [1/3] Loss: 90.874 | Train Acc: 0.00%\n",
            "Epoch [1/3] Loss: 96.660 | Train Acc: 0.00%\n",
            "Epoch [1/3] Loss: 102.620 | Train Acc: 0.00%\n",
            "Epoch [1/3] Loss: 108.550 | Train Acc: 0.00%\n",
            "Epoch [1/3] Loss: 114.440 | Train Acc: 0.00%\n",
            "Epoch [1/3] Loss: 120.330 | Train Acc: 0.00%\n",
            "Epoch [1/3] Loss: 126.379 | Train Acc: 0.15%\n",
            "Epoch [1/3] Loss: 132.267 | Train Acc: 0.14%\n",
            "Epoch [1/3] Loss: 138.235 | Train Acc: 0.14%\n",
            "Epoch [1/3] Loss: 143.927 | Train Acc: 0.13%\n",
            "Epoch [1/3] Loss: 149.973 | Train Acc: 0.12%\n",
            "Epoch [1/3] Loss: 155.891 | Train Acc: 0.12%\n",
            "Epoch [1/3] Loss: 161.688 | Train Acc: 0.12%\n",
            "Epoch [1/3] Loss: 167.631 | Train Acc: 0.33%\n",
            "Epoch [1/3] Loss: 173.499 | Train Acc: 0.32%\n",
            "Epoch [1/3] Loss: 179.290 | Train Acc: 0.42%\n",
            "Epoch [1/3] Loss: 184.795 | Train Acc: 0.50%\n",
            "Epoch [1/3] Loss: 190.846 | Train Acc: 0.49%\n",
            "Epoch [1/3] Loss: 196.609 | Train Acc: 0.47%\n",
            "Epoch [1/3] Loss: 202.510 | Train Acc: 0.46%\n",
            "Epoch [1/3] Loss: 208.295 | Train Acc: 0.45%\n",
            "Epoch [1/3] Loss: 213.812 | Train Acc: 0.52%\n",
            "Epoch [1/3] Loss: 219.726 | Train Acc: 0.51%\n",
            "Epoch [1/3] Loss: 225.569 | Train Acc: 0.49%\n",
            "Epoch [1/3] Loss: 231.364 | Train Acc: 0.48%\n",
            "Epoch [1/3] Loss: 237.094 | Train Acc: 0.47%\n",
            "Epoch [1/3] Loss: 242.856 | Train Acc: 0.53%\n",
            "Epoch [1/3] Loss: 248.368 | Train Acc: 0.52%\n",
            "Epoch [1/3] Loss: 253.933 | Train Acc: 0.51%\n",
            "Epoch [1/3] Loss: 259.603 | Train Acc: 0.50%\n",
            "Epoch [1/3] Loss: 265.356 | Train Acc: 0.49%\n",
            "Epoch [1/3] Loss: 271.246 | Train Acc: 0.54%\n",
            "Epoch [1/3] Loss: 276.919 | Train Acc: 0.53%\n",
            "Epoch [1/3] Loss: 282.573 | Train Acc: 0.52%\n",
            "Epoch [1/3] Loss: 288.214 | Train Acc: 0.57%\n",
            "Epoch [1/3] Loss: 293.846 | Train Acc: 0.56%\n",
            "Epoch [1/3] Loss: 299.447 | Train Acc: 0.55%\n",
            "Epoch [1/3] Loss: 305.262 | Train Acc: 0.54%\n",
            "Epoch [1/3] Loss: 310.996 | Train Acc: 0.53%\n",
            "Epoch [1/3] Loss: 316.563 | Train Acc: 0.52%\n",
            "Epoch [1/3] Loss: 322.197 | Train Acc: 0.51%\n",
            "Epoch [1/3] Loss: 327.828 | Train Acc: 0.50%\n",
            "Epoch [1/3] Loss: 333.512 | Train Acc: 0.49%\n",
            "Epoch [1/3] Loss: 339.224 | Train Acc: 0.48%\n",
            "Epoch [1/3] Loss: 344.714 | Train Acc: 0.74%\n",
            "Epoch [1/3] Loss: 350.059 | Train Acc: 0.78%\n",
            "Epoch [1/3] Loss: 355.749 | Train Acc: 0.77%\n",
            "Epoch [1/3] Loss: 361.259 | Train Acc: 0.76%\n",
            "Epoch [1/3] Loss: 366.971 | Train Acc: 0.79%\n",
            "Epoch [1/3] Loss: 372.689 | Train Acc: 0.78%\n",
            "Epoch [1/3] Loss: 378.040 | Train Acc: 0.82%\n",
            "Epoch [1/3] Loss: 383.394 | Train Acc: 0.90%\n",
            "Epoch [1/3] Loss: 388.945 | Train Acc: 0.93%\n",
            "Epoch [1/3] Loss: 394.517 | Train Acc: 1.06%\n",
            "Epoch [1/3] Loss: 400.112 | Train Acc: 1.09%\n",
            "Epoch [1/3] Loss: 405.552 | Train Acc: 1.07%\n",
            "Epoch [1/3] Loss: 411.289 | Train Acc: 1.10%\n",
            "Epoch [1/3] Loss: 416.679 | Train Acc: 1.09%\n",
            "Epoch [1/3] Loss: 422.342 | Train Acc: 1.07%\n",
            "Epoch [1/3] Loss: 427.950 | Train Acc: 1.10%\n",
            "Epoch [1/3] Loss: 433.665 | Train Acc: 1.12%\n",
            "Epoch [1/3] Loss: 439.266 | Train Acc: 1.11%\n",
            "Epoch [1/3] Loss: 444.818 | Train Acc: 1.10%\n",
            "Epoch [1/3] Loss: 450.426 | Train Acc: 1.12%\n",
            "Epoch [1/3] Loss: 455.724 | Train Acc: 1.19%\n",
            "Epoch [1/3] Loss: 461.229 | Train Acc: 1.25%\n",
            "Epoch [1/3] Loss: 466.746 | Train Acc: 1.27%\n",
            "Epoch [1/3] Loss: 472.239 | Train Acc: 1.26%\n",
            "Epoch [1/3] Loss: 477.957 | Train Acc: 1.24%\n",
            "Epoch [1/3] Loss: 483.355 | Train Acc: 1.30%\n",
            "Epoch [1/3] Loss: 489.056 | Train Acc: 1.29%\n",
            "Epoch [1/3] Loss: 494.579 | Train Acc: 1.27%\n",
            "Epoch [1/3] Loss: 499.911 | Train Acc: 1.26%\n",
            "Epoch [1/3] Loss: 505.694 | Train Acc: 1.24%\n",
            "Epoch [1/3] Loss: 511.364 | Train Acc: 1.23%\n",
            "Epoch [1/3] Loss: 516.893 | Train Acc: 1.28%\n",
            "Epoch [1/3] Loss: 522.412 | Train Acc: 1.34%\n",
            "Epoch [1/3] Loss: 527.857 | Train Acc: 1.46%\n",
            "Epoch [1/3] Loss: 533.300 | Train Acc: 1.48%\n",
            "Epoch [1/3] Loss: 538.636 | Train Acc: 1.56%\n",
            "Epoch [1/3] Loss: 544.085 | Train Acc: 1.55%\n",
            "Epoch [1/3] Loss: 549.651 | Train Acc: 1.53%\n",
            "Epoch [1/3] Loss: 555.070 | Train Acc: 1.55%\n",
            "Epoch [1/3] Loss: 560.531 | Train Acc: 1.53%\n",
            "Epoch [1/3] Loss: 565.765 | Train Acc: 1.61%\n",
            "Epoch [1/3] Loss: 571.072 | Train Acc: 1.62%\n",
            "Epoch [1/3] Loss: 576.333 | Train Acc: 1.64%\n",
            "Epoch [1/3] Loss: 581.913 | Train Acc: 1.69%\n",
            "Epoch [1/3] Loss: 587.315 | Train Acc: 1.76%\n",
            "Epoch [1/3] Loss: 592.659 | Train Acc: 1.74%\n",
            "Epoch [1/3] Loss: 597.966 | Train Acc: 1.76%\n",
            "Epoch [1/3] Loss: 603.553 | Train Acc: 1.80%\n",
            "Epoch [1/3] Loss: 609.013 | Train Acc: 1.78%\n",
            "Epoch [1/3] Loss: 614.469 | Train Acc: 1.82%\n",
            "Epoch [1/3] Loss: 619.741 | Train Acc: 1.89%\n",
            "Epoch [1/3] Loss: 625.170 | Train Acc: 1.93%\n",
            "Epoch [1/3] Loss: 630.629 | Train Acc: 2.03%\n",
            "Epoch [1/3] Loss: 636.109 | Train Acc: 2.06%\n",
            "Epoch [1/3] Loss: 641.462 | Train Acc: 2.10%\n",
            "Epoch [1/3] Loss: 646.729 | Train Acc: 2.17%\n",
            "Epoch [1/3] Loss: 652.089 | Train Acc: 2.17%\n",
            "Epoch [1/3] Loss: 657.355 | Train Acc: 2.18%\n",
            "Epoch [1/3] Loss: 662.383 | Train Acc: 2.30%\n",
            "Epoch [1/3] Loss: 667.701 | Train Acc: 2.28%\n",
            "Epoch [1/3] Loss: 673.068 | Train Acc: 2.31%\n",
            "Epoch [1/3] Loss: 678.293 | Train Acc: 2.34%\n",
            "Epoch [1/3] Loss: 683.689 | Train Acc: 2.32%\n",
            "Epoch [1/3] Loss: 688.760 | Train Acc: 2.36%\n",
            "Epoch [1/3] Loss: 694.136 | Train Acc: 2.34%\n",
            "Epoch [1/3] Loss: 699.641 | Train Acc: 2.32%\n",
            "Epoch [1/3] Loss: 705.180 | Train Acc: 2.30%\n",
            "Epoch [1/3] Loss: 710.535 | Train Acc: 2.36%\n",
            "Epoch [1/3] Loss: 715.854 | Train Acc: 2.36%\n",
            "Epoch [1/3] Loss: 720.971 | Train Acc: 2.47%\n",
            "Epoch [1/3] Loss: 726.099 | Train Acc: 2.50%\n",
            "Epoch [1/3] Loss: 731.321 | Train Acc: 2.50%\n",
            "Epoch [1/3] Loss: 736.794 | Train Acc: 2.53%\n",
            "Epoch [1/3] Loss: 741.919 | Train Acc: 2.63%\n",
            "Epoch [1/3] Loss: 747.135 | Train Acc: 2.63%\n",
            "Epoch [1/3] Loss: 752.452 | Train Acc: 2.61%\n",
            "Epoch [1/3] Loss: 757.538 | Train Acc: 2.66%\n",
            "Epoch [1/3] Loss: 762.881 | Train Acc: 2.73%\n",
            "Epoch [1/3] Loss: 768.087 | Train Acc: 2.76%\n",
            "Epoch [1/3] Loss: 773.441 | Train Acc: 2.79%\n",
            "Epoch [1/3] Loss: 778.444 | Train Acc: 2.90%\n",
            "Epoch [1/3] Loss: 783.413 | Train Acc: 2.97%\n",
            "Epoch [1/3] Loss: 788.232 | Train Acc: 2.99%\n",
            "Epoch [1/3] Loss: 793.204 | Train Acc: 3.01%\n",
            "Epoch [1/3] Loss: 798.483 | Train Acc: 2.99%\n",
            "Epoch [1/3] Loss: 803.485 | Train Acc: 3.02%\n",
            "Epoch [1/3] Loss: 808.751 | Train Acc: 3.00%\n",
            "Epoch [1/3] Loss: 813.553 | Train Acc: 3.10%\n",
            "Epoch [1/3] Loss: 818.536 | Train Acc: 3.15%\n",
            "Epoch [1/3] Loss: 823.590 | Train Acc: 3.19%\n",
            "Epoch [1/3] Loss: 828.683 | Train Acc: 3.19%\n",
            "Epoch [1/3] Loss: 833.846 | Train Acc: 3.21%\n",
            "Epoch [1/3] Loss: 838.790 | Train Acc: 3.29%\n",
            "Epoch [1/3] Loss: 843.947 | Train Acc: 3.31%\n",
            "Epoch [1/3] Loss: 848.810 | Train Acc: 3.35%\n",
            "Epoch [1/3] Loss: 853.903 | Train Acc: 3.39%\n",
            "Epoch [1/3] Loss: 858.922 | Train Acc: 3.41%\n",
            "Epoch [1/3] Loss: 863.986 | Train Acc: 3.45%\n",
            "Epoch [1/3] Loss: 869.308 | Train Acc: 3.46%\n",
            "Epoch [1/3] Loss: 874.361 | Train Acc: 3.50%\n",
            "Epoch [1/3] Loss: 879.383 | Train Acc: 3.52%\n",
            "Epoch [1/3] Loss: 884.736 | Train Acc: 3.54%\n",
            "Epoch [1/3] Loss: 889.998 | Train Acc: 3.53%\n",
            "Epoch [1/3] Loss: 894.973 | Train Acc: 3.61%\n",
            "Epoch [1/3] Loss: 900.150 | Train Acc: 3.60%\n",
            "Epoch [1/3] Loss: 905.258 | Train Acc: 3.64%\n",
            "Epoch [1/3] Loss: 910.460 | Train Acc: 3.64%\n",
            "Epoch [1/3] Loss: 915.379 | Train Acc: 3.67%\n",
            "Epoch [1/3] Loss: 920.556 | Train Acc: 3.65%\n",
            "Epoch [1/3] Loss: 925.961 | Train Acc: 3.65%\n",
            "Epoch [1/3] Loss: 931.031 | Train Acc: 3.64%\n",
            "Epoch [1/3] Loss: 936.146 | Train Acc: 3.64%\n",
            "Epoch [1/3] Loss: 941.505 | Train Acc: 3.62%\n",
            "Epoch [1/3] Loss: 946.439 | Train Acc: 3.65%\n",
            "Epoch [1/3] Loss: 951.418 | Train Acc: 3.70%\n",
            "Epoch [1/3] Loss: 956.789 | Train Acc: 3.72%\n",
            "Epoch [1/3] Loss: 961.802 | Train Acc: 3.77%\n",
            "Epoch [1/3] Loss: 966.921 | Train Acc: 3.82%\n",
            "Epoch [1/3] Loss: 972.122 | Train Acc: 3.81%\n",
            "Epoch [1/3] Loss: 977.040 | Train Acc: 3.81%\n",
            "Epoch [1/3] Loss: 982.072 | Train Acc: 3.86%\n",
            "Epoch [1/3] Loss: 987.145 | Train Acc: 3.84%\n",
            "Epoch [1/3] Loss: 992.230 | Train Acc: 3.90%\n",
            "Epoch [1/3] Loss: 997.296 | Train Acc: 3.93%\n",
            "Epoch [1/3] Loss: 1002.336 | Train Acc: 3.98%\n",
            "Epoch [1/3] Loss: 1007.575 | Train Acc: 3.96%\n",
            "Epoch [1/3] Loss: 1012.717 | Train Acc: 4.00%\n",
            "Epoch [1/3] Loss: 1018.013 | Train Acc: 4.03%\n",
            "Epoch [1/3] Loss: 1023.294 | Train Acc: 4.03%\n",
            "Epoch [1/3] Loss: 1028.212 | Train Acc: 4.07%\n",
            "Epoch [1/3] Loss: 1033.378 | Train Acc: 4.07%\n",
            "Epoch [1/3] Loss: 1038.155 | Train Acc: 4.11%\n",
            "Epoch [1/3] Loss: 1043.468 | Train Acc: 4.12%\n",
            "Epoch [1/3] Loss: 1048.538 | Train Acc: 4.18%\n",
            "Epoch [1/3] Loss: 1053.406 | Train Acc: 4.19%\n",
            "Epoch [1/3] Loss: 1058.467 | Train Acc: 4.20%\n",
            "Epoch [1/3] Loss: 1063.570 | Train Acc: 4.21%\n",
            "Epoch [1/3] Loss: 1068.537 | Train Acc: 4.24%\n",
            "Epoch [1/3] Loss: 1073.128 | Train Acc: 4.33%\n",
            "Epoch [1/3] Loss: 1078.280 | Train Acc: 4.31%\n",
            "Epoch [1/3] Loss: 1083.025 | Train Acc: 4.33%\n",
            "Epoch [1/3] Loss: 1087.861 | Train Acc: 4.34%\n",
            "Epoch [1/3] Loss: 1092.981 | Train Acc: 4.37%\n",
            "Epoch [1/3] Loss: 1098.011 | Train Acc: 4.39%\n",
            "Epoch [1/3] Loss: 1102.972 | Train Acc: 4.43%\n",
            "Epoch [1/3] Loss: 1107.823 | Train Acc: 4.44%\n",
            "Epoch [1/3] Loss: 1112.796 | Train Acc: 4.44%\n",
            "Epoch [1/3] Loss: 1117.872 | Train Acc: 4.43%\n",
            "Epoch [1/3] Loss: 1122.563 | Train Acc: 4.48%\n",
            "Epoch [1/3] Loss: 1127.425 | Train Acc: 4.51%\n",
            "Epoch [1/3] Loss: 1132.244 | Train Acc: 4.52%\n",
            "Epoch [1/3] Loss: 1137.142 | Train Acc: 4.54%\n",
            "Epoch [1/3] Loss: 1142.091 | Train Acc: 4.56%\n",
            "Epoch [1/3] Loss: 1146.832 | Train Acc: 4.60%\n",
            "Epoch [1/3] Loss: 1151.449 | Train Acc: 4.67%\n",
            "Epoch [1/3] Loss: 1156.410 | Train Acc: 4.72%\n",
            "Epoch [1/3] Loss: 1161.135 | Train Acc: 4.72%\n",
            "Epoch [1/3] Loss: 1166.059 | Train Acc: 4.76%\n",
            "Epoch [1/3] Loss: 1171.147 | Train Acc: 4.77%\n",
            "Epoch [1/3] Loss: 1175.772 | Train Acc: 4.82%\n",
            "Epoch [1/3] Loss: 1180.380 | Train Acc: 4.85%\n",
            "Epoch [1/3] Loss: 1185.418 | Train Acc: 4.84%\n",
            "Epoch [1/3] Loss: 1190.218 | Train Acc: 4.86%\n",
            "Epoch [1/3] Loss: 1194.688 | Train Acc: 4.91%\n",
            "Epoch [1/3] Loss: 1199.528 | Train Acc: 4.95%\n",
            "Epoch [1/3] Loss: 1204.330 | Train Acc: 4.95%\n",
            "Epoch [1/3] Loss: 1209.352 | Train Acc: 4.96%\n",
            "Epoch [1/3] Loss: 1214.213 | Train Acc: 4.98%\n",
            "Epoch [1/3] Loss: 1219.201 | Train Acc: 5.01%\n",
            "Epoch [1/3] Loss: 1224.215 | Train Acc: 5.03%\n",
            "Epoch [1/3] Loss: 1228.814 | Train Acc: 5.06%\n",
            "Epoch [1/3] Loss: 1233.652 | Train Acc: 5.10%\n",
            "Epoch [1/3] Loss: 1238.188 | Train Acc: 5.14%\n",
            "Epoch [1/3] Loss: 1243.102 | Train Acc: 5.15%\n",
            "Epoch [1/3] Loss: 1248.346 | Train Acc: 5.12%\n",
            "Epoch [1/3] Loss: 1253.266 | Train Acc: 5.14%\n",
            "Epoch [1/3] Loss: 1258.134 | Train Acc: 5.16%\n",
            "Epoch [1/3] Loss: 1263.184 | Train Acc: 5.18%\n",
            "Epoch [1/3] Loss: 1268.082 | Train Acc: 5.18%\n",
            "Epoch [1/3] Loss: 1272.625 | Train Acc: 5.21%\n",
            "Epoch [1/3] Loss: 1277.226 | Train Acc: 5.26%\n",
            "Epoch [1/3] Loss: 1281.893 | Train Acc: 5.30%\n",
            "Epoch [1/3] Loss: 1286.728 | Train Acc: 5.32%\n",
            "Epoch [1/3] Loss: 1291.826 | Train Acc: 5.32%\n",
            "Epoch [1/3] Loss: 1296.503 | Train Acc: 5.34%\n",
            "Epoch [1/3] Loss: 1301.217 | Train Acc: 5.38%\n",
            "Epoch [1/3] Loss: 1305.744 | Train Acc: 5.43%\n",
            "Epoch [1/3] Loss: 1310.633 | Train Acc: 5.45%\n",
            "Epoch [1/3] Loss: 1315.371 | Train Acc: 5.50%\n",
            "Epoch [1/3] Loss: 1320.140 | Train Acc: 5.54%\n",
            "Epoch [1/3] Loss: 1325.148 | Train Acc: 5.55%\n",
            "Epoch [1/3] Loss: 1329.870 | Train Acc: 5.55%\n",
            "Epoch [1/3] Loss: 1334.653 | Train Acc: 5.55%\n",
            "Epoch [1/3] Loss: 1339.526 | Train Acc: 5.54%\n",
            "Epoch [1/3] Loss: 1344.289 | Train Acc: 5.56%\n",
            "Epoch [1/3] Loss: 1348.974 | Train Acc: 5.59%\n",
            "Epoch [1/3] Loss: 1353.774 | Train Acc: 5.62%\n",
            "Epoch [1/3] Loss: 1358.608 | Train Acc: 5.65%\n",
            "Epoch [1/3] Loss: 1363.528 | Train Acc: 5.67%\n",
            "Epoch [1/3] Loss: 1368.340 | Train Acc: 5.69%\n",
            "Epoch [1/3] Loss: 1373.203 | Train Acc: 5.72%\n",
            "Epoch [1/3] Loss: 1377.976 | Train Acc: 5.71%\n",
            "Epoch [1/3] Loss: 1382.735 | Train Acc: 5.72%\n",
            "Epoch [1/3] Loss: 1387.275 | Train Acc: 5.76%\n",
            "Epoch [1/3] Loss: 1391.876 | Train Acc: 5.79%\n",
            "Epoch [1/3] Loss: 1396.517 | Train Acc: 5.80%\n",
            "Epoch [1/3] Loss: 1401.278 | Train Acc: 5.83%\n",
            "Epoch [1/3] Loss: 1406.004 | Train Acc: 5.86%\n",
            "Epoch [1/3] Loss: 1410.738 | Train Acc: 5.86%\n",
            "Epoch [1/3] Loss: 1415.501 | Train Acc: 5.87%\n",
            "Epoch [1/3] Loss: 1420.507 | Train Acc: 5.88%\n",
            "Epoch [1/3] Loss: 1424.994 | Train Acc: 5.94%\n",
            "Epoch [1/3] Loss: 1429.552 | Train Acc: 5.97%\n",
            "Epoch [1/3] Loss: 1434.374 | Train Acc: 5.99%\n",
            "Epoch [1/3] Loss: 1439.082 | Train Acc: 6.02%\n",
            "Epoch [1/3] Loss: 1443.608 | Train Acc: 6.07%\n",
            "Epoch [1/3] Loss: 1448.151 | Train Acc: 6.10%\n",
            "Epoch [1/3] Loss: 1452.938 | Train Acc: 6.11%\n",
            "Epoch [1/3] Loss: 1457.582 | Train Acc: 6.10%\n",
            "Epoch [1/3] Loss: 1462.206 | Train Acc: 6.15%\n",
            "Epoch [1/3] Loss: 1466.783 | Train Acc: 6.16%\n",
            "Epoch [1/3] Loss: 1471.355 | Train Acc: 6.22%\n",
            "Epoch [1/3] Loss: 1476.040 | Train Acc: 6.22%\n",
            "Epoch [1/3] Loss: 1480.503 | Train Acc: 6.24%\n",
            "Epoch [1/3] Loss: 1485.369 | Train Acc: 6.25%\n",
            "Epoch [1/3] Loss: 1489.961 | Train Acc: 6.28%\n",
            "Epoch [1/3] Loss: 1494.525 | Train Acc: 6.33%\n",
            "Epoch [1/3] Loss: 1499.054 | Train Acc: 6.37%\n",
            "Epoch [1/3] Loss: 1503.681 | Train Acc: 6.40%\n",
            "Epoch [1/3] Loss: 1508.499 | Train Acc: 6.41%\n",
            "Epoch [1/3] Loss: 1512.789 | Train Acc: 6.46%\n",
            "Epoch [1/3] Loss: 1517.427 | Train Acc: 6.48%\n",
            "Epoch [1/3] Loss: 1522.215 | Train Acc: 6.49%\n",
            "Epoch [1/3] Loss: 1526.873 | Train Acc: 6.51%\n",
            "Epoch [1/3] Loss: 1531.436 | Train Acc: 6.56%\n",
            "Epoch [1/3] Loss: 1536.164 | Train Acc: 6.56%\n",
            "Epoch [1/3] Loss: 1540.783 | Train Acc: 6.58%\n",
            "Epoch [1/3] Loss: 1545.244 | Train Acc: 6.62%\n",
            "Epoch [1/3] Loss: 1549.837 | Train Acc: 6.67%\n",
            "Epoch [1/3] Loss: 1554.695 | Train Acc: 6.66%\n",
            "Epoch [1/3] Loss: 1559.417 | Train Acc: 6.67%\n",
            "Epoch [1/3] Loss: 1563.766 | Train Acc: 6.68%\n",
            "Epoch [1/3] Loss: 1568.168 | Train Acc: 6.68%\n",
            "Epoch [1/3] Loss: 1572.888 | Train Acc: 6.69%\n",
            "Epoch [1/3] Loss: 1577.653 | Train Acc: 6.72%\n",
            "Epoch [1/3] Loss: 1582.385 | Train Acc: 6.72%\n",
            "Epoch [2/3] Loss: 4.326 | Train Acc: 18.75%\n",
            "Epoch [2/3] Loss: 8.378 | Train Acc: 23.44%\n",
            "Epoch [2/3] Loss: 12.429 | Train Acc: 28.12%\n",
            "Epoch [2/3] Loss: 16.582 | Train Acc: 26.56%\n",
            "Epoch [2/3] Loss: 20.834 | Train Acc: 26.25%\n",
            "Epoch [2/3] Loss: 24.997 | Train Acc: 26.56%\n",
            "Epoch [2/3] Loss: 29.093 | Train Acc: 27.23%\n",
            "Epoch [2/3] Loss: 33.474 | Train Acc: 26.17%\n",
            "Epoch [2/3] Loss: 37.844 | Train Acc: 25.00%\n",
            "Epoch [2/3] Loss: 42.000 | Train Acc: 25.00%\n",
            "Epoch [2/3] Loss: 46.319 | Train Acc: 24.72%\n",
            "Epoch [2/3] Loss: 50.654 | Train Acc: 24.22%\n",
            "Epoch [2/3] Loss: 55.015 | Train Acc: 23.32%\n",
            "Epoch [2/3] Loss: 59.532 | Train Acc: 22.77%\n",
            "Epoch [2/3] Loss: 63.398 | Train Acc: 23.75%\n",
            "Epoch [2/3] Loss: 67.534 | Train Acc: 24.22%\n",
            "Epoch [2/3] Loss: 71.687 | Train Acc: 24.45%\n",
            "Epoch [2/3] Loss: 75.639 | Train Acc: 24.83%\n",
            "Epoch [2/3] Loss: 79.801 | Train Acc: 25.00%\n",
            "Epoch [2/3] Loss: 83.741 | Train Acc: 24.69%\n",
            "Epoch [2/3] Loss: 87.813 | Train Acc: 25.30%\n",
            "Epoch [2/3] Loss: 92.123 | Train Acc: 25.14%\n",
            "Epoch [2/3] Loss: 96.351 | Train Acc: 24.73%\n",
            "Epoch [2/3] Loss: 100.869 | Train Acc: 24.35%\n",
            "Epoch [2/3] Loss: 104.975 | Train Acc: 24.50%\n",
            "Epoch [2/3] Loss: 109.214 | Train Acc: 24.52%\n",
            "Epoch [2/3] Loss: 113.376 | Train Acc: 24.42%\n",
            "Epoch [2/3] Loss: 117.427 | Train Acc: 24.55%\n",
            "Epoch [2/3] Loss: 121.514 | Train Acc: 24.89%\n",
            "Epoch [2/3] Loss: 125.518 | Train Acc: 25.10%\n",
            "Epoch [2/3] Loss: 129.863 | Train Acc: 24.80%\n",
            "Epoch [2/3] Loss: 134.301 | Train Acc: 24.32%\n",
            "Epoch [2/3] Loss: 138.785 | Train Acc: 24.15%\n",
            "Epoch [2/3] Loss: 142.891 | Train Acc: 24.26%\n",
            "Epoch [2/3] Loss: 146.916 | Train Acc: 24.20%\n",
            "Epoch [2/3] Loss: 151.211 | Train Acc: 23.87%\n",
            "Epoch [2/3] Loss: 155.170 | Train Acc: 23.82%\n",
            "Epoch [2/3] Loss: 159.341 | Train Acc: 24.18%\n",
            "Epoch [2/3] Loss: 163.434 | Train Acc: 23.96%\n",
            "Epoch [2/3] Loss: 167.710 | Train Acc: 24.06%\n",
            "Epoch [2/3] Loss: 172.244 | Train Acc: 23.93%\n",
            "Epoch [2/3] Loss: 176.384 | Train Acc: 23.81%\n",
            "Epoch [2/3] Loss: 180.541 | Train Acc: 23.76%\n",
            "Epoch [2/3] Loss: 184.623 | Train Acc: 23.79%\n",
            "Epoch [2/3] Loss: 188.674 | Train Acc: 23.89%\n",
            "Epoch [2/3] Loss: 192.455 | Train Acc: 24.12%\n",
            "Epoch [2/3] Loss: 196.316 | Train Acc: 24.34%\n",
            "Epoch [2/3] Loss: 200.466 | Train Acc: 24.35%\n",
            "Epoch [2/3] Loss: 204.686 | Train Acc: 24.49%\n",
            "Epoch [2/3] Loss: 208.796 | Train Acc: 24.56%\n",
            "Epoch [2/3] Loss: 212.974 | Train Acc: 24.33%\n",
            "Epoch [2/3] Loss: 217.155 | Train Acc: 24.34%\n",
            "Epoch [2/3] Loss: 221.254 | Train Acc: 24.41%\n",
            "Epoch [2/3] Loss: 225.203 | Train Acc: 24.48%\n",
            "Epoch [2/3] Loss: 229.601 | Train Acc: 24.43%\n",
            "Epoch [2/3] Loss: 233.559 | Train Acc: 24.55%\n",
            "Epoch [2/3] Loss: 237.582 | Train Acc: 24.67%\n",
            "Epoch [2/3] Loss: 241.635 | Train Acc: 24.68%\n",
            "Epoch [2/3] Loss: 245.556 | Train Acc: 24.79%\n",
            "Epoch [2/3] Loss: 249.880 | Train Acc: 24.69%\n",
            "Epoch [2/3] Loss: 254.185 | Train Acc: 24.54%\n",
            "Epoch [2/3] Loss: 258.381 | Train Acc: 24.55%\n",
            "Epoch [2/3] Loss: 262.479 | Train Acc: 24.40%\n",
            "Epoch [2/3] Loss: 266.672 | Train Acc: 24.32%\n",
            "Epoch [2/3] Loss: 270.746 | Train Acc: 24.33%\n",
            "Epoch [2/3] Loss: 274.812 | Train Acc: 24.43%\n",
            "Epoch [2/3] Loss: 278.703 | Train Acc: 24.58%\n",
            "Epoch [2/3] Loss: 282.638 | Train Acc: 24.68%\n",
            "Epoch [2/3] Loss: 286.586 | Train Acc: 24.73%\n",
            "Epoch [2/3] Loss: 290.573 | Train Acc: 24.87%\n",
            "Epoch [2/3] Loss: 294.798 | Train Acc: 24.74%\n",
            "Epoch [2/3] Loss: 298.633 | Train Acc: 24.91%\n",
            "Epoch [2/3] Loss: 302.782 | Train Acc: 24.96%\n",
            "Epoch [2/3] Loss: 306.568 | Train Acc: 25.08%\n",
            "Epoch [2/3] Loss: 310.454 | Train Acc: 25.25%\n",
            "Epoch [2/3] Loss: 314.816 | Train Acc: 25.12%\n",
            "Epoch [2/3] Loss: 318.753 | Train Acc: 25.16%\n",
            "Epoch [2/3] Loss: 322.803 | Train Acc: 25.12%\n",
            "Epoch [2/3] Loss: 326.864 | Train Acc: 25.20%\n",
            "Epoch [2/3] Loss: 331.095 | Train Acc: 25.12%\n",
            "Epoch [2/3] Loss: 335.018 | Train Acc: 25.08%\n",
            "Epoch [2/3] Loss: 339.242 | Train Acc: 24.96%\n",
            "Epoch [2/3] Loss: 343.247 | Train Acc: 24.92%\n",
            "Epoch [2/3] Loss: 347.482 | Train Acc: 25.00%\n",
            "Epoch [2/3] Loss: 351.325 | Train Acc: 25.04%\n",
            "Epoch [2/3] Loss: 355.436 | Train Acc: 25.11%\n",
            "Epoch [2/3] Loss: 359.396 | Train Acc: 25.04%\n",
            "Epoch [2/3] Loss: 363.155 | Train Acc: 25.18%\n",
            "Epoch [2/3] Loss: 367.552 | Train Acc: 25.11%\n",
            "Epoch [2/3] Loss: 371.435 | Train Acc: 25.10%\n",
            "Epoch [2/3] Loss: 375.350 | Train Acc: 25.24%\n",
            "Epoch [2/3] Loss: 379.449 | Train Acc: 25.27%\n",
            "Epoch [2/3] Loss: 383.805 | Train Acc: 25.13%\n",
            "Epoch [2/3] Loss: 387.584 | Train Acc: 25.23%\n",
            "Epoch [2/3] Loss: 391.745 | Train Acc: 25.20%\n",
            "Epoch [2/3] Loss: 395.498 | Train Acc: 25.23%\n",
            "Epoch [2/3] Loss: 399.643 | Train Acc: 25.19%\n",
            "Epoch [2/3] Loss: 404.324 | Train Acc: 25.06%\n",
            "Epoch [2/3] Loss: 408.290 | Train Acc: 25.16%\n",
            "Epoch [2/3] Loss: 412.467 | Train Acc: 25.09%\n",
            "Epoch [2/3] Loss: 416.356 | Train Acc: 25.12%\n",
            "Epoch [2/3] Loss: 420.308 | Train Acc: 25.21%\n",
            "Epoch [2/3] Loss: 424.273 | Train Acc: 25.24%\n",
            "Epoch [2/3] Loss: 428.382 | Train Acc: 25.27%\n",
            "Epoch [2/3] Loss: 432.433 | Train Acc: 25.24%\n",
            "Epoch [2/3] Loss: 436.340 | Train Acc: 25.27%\n",
            "Epoch [2/3] Loss: 440.136 | Train Acc: 25.29%\n",
            "Epoch [2/3] Loss: 444.562 | Train Acc: 25.14%\n",
            "Epoch [2/3] Loss: 448.659 | Train Acc: 25.09%\n",
            "Epoch [2/3] Loss: 452.538 | Train Acc: 25.11%\n",
            "Epoch [2/3] Loss: 456.638 | Train Acc: 25.14%\n",
            "Epoch [2/3] Loss: 460.498 | Train Acc: 25.14%\n",
            "Epoch [2/3] Loss: 464.238 | Train Acc: 25.30%\n",
            "Epoch [2/3] Loss: 467.982 | Train Acc: 25.36%\n",
            "Epoch [2/3] Loss: 472.109 | Train Acc: 25.41%\n",
            "Epoch [2/3] Loss: 476.193 | Train Acc: 25.35%\n",
            "Epoch [2/3] Loss: 479.964 | Train Acc: 25.35%\n",
            "Epoch [2/3] Loss: 484.051 | Train Acc: 25.37%\n",
            "Epoch [2/3] Loss: 488.221 | Train Acc: 25.42%\n",
            "Epoch [2/3] Loss: 491.829 | Train Acc: 25.57%\n",
            "Epoch [2/3] Loss: 495.886 | Train Acc: 25.62%\n",
            "Epoch [2/3] Loss: 499.667 | Train Acc: 25.67%\n",
            "Epoch [2/3] Loss: 503.832 | Train Acc: 25.58%\n",
            "Epoch [2/3] Loss: 507.962 | Train Acc: 25.58%\n",
            "Epoch [2/3] Loss: 511.832 | Train Acc: 25.60%\n",
            "Epoch [2/3] Loss: 515.874 | Train Acc: 25.64%\n",
            "Epoch [2/3] Loss: 520.126 | Train Acc: 25.59%\n",
            "Epoch [2/3] Loss: 524.094 | Train Acc: 25.61%\n",
            "Epoch [2/3] Loss: 527.883 | Train Acc: 25.63%\n",
            "Epoch [2/3] Loss: 531.632 | Train Acc: 25.62%\n",
            "Epoch [2/3] Loss: 535.172 | Train Acc: 25.88%\n",
            "Epoch [2/3] Loss: 539.427 | Train Acc: 25.90%\n",
            "Epoch [2/3] Loss: 543.518 | Train Acc: 25.89%\n",
            "Epoch [2/3] Loss: 547.306 | Train Acc: 25.89%\n",
            "Epoch [2/3] Loss: 551.268 | Train Acc: 25.90%\n",
            "Epoch [2/3] Loss: 555.075 | Train Acc: 25.99%\n",
            "Epoch [2/3] Loss: 559.036 | Train Acc: 26.03%\n",
            "Epoch [2/3] Loss: 562.956 | Train Acc: 26.00%\n",
            "Epoch [2/3] Loss: 567.005 | Train Acc: 25.97%\n",
            "Epoch [2/3] Loss: 570.841 | Train Acc: 25.98%\n",
            "Epoch [2/3] Loss: 574.760 | Train Acc: 26.00%\n",
            "Epoch [2/3] Loss: 578.459 | Train Acc: 26.12%\n",
            "Epoch [2/3] Loss: 582.258 | Train Acc: 26.16%\n",
            "Epoch [2/3] Loss: 586.144 | Train Acc: 26.13%\n",
            "Epoch [2/3] Loss: 590.133 | Train Acc: 26.10%\n",
            "Epoch [2/3] Loss: 594.012 | Train Acc: 26.16%\n",
            "Epoch [2/3] Loss: 597.868 | Train Acc: 26.21%\n",
            "Epoch [2/3] Loss: 601.649 | Train Acc: 26.25%\n",
            "Epoch [2/3] Loss: 605.842 | Train Acc: 26.17%\n",
            "Epoch [2/3] Loss: 609.972 | Train Acc: 26.15%\n",
            "Epoch [2/3] Loss: 613.492 | Train Acc: 26.16%\n",
            "Epoch [2/3] Loss: 617.038 | Train Acc: 26.27%\n",
            "Epoch [2/3] Loss: 621.004 | Train Acc: 26.23%\n",
            "Epoch [2/3] Loss: 624.443 | Train Acc: 26.30%\n",
            "Epoch [2/3] Loss: 628.309 | Train Acc: 26.33%\n",
            "Epoch [2/3] Loss: 631.963 | Train Acc: 26.38%\n",
            "Epoch [2/3] Loss: 635.999 | Train Acc: 26.37%\n",
            "Epoch [2/3] Loss: 640.088 | Train Acc: 26.40%\n",
            "Epoch [2/3] Loss: 643.962 | Train Acc: 26.45%\n",
            "Epoch [2/3] Loss: 647.737 | Train Acc: 26.45%\n",
            "Epoch [2/3] Loss: 651.776 | Train Acc: 26.44%\n",
            "Epoch [2/3] Loss: 655.635 | Train Acc: 26.47%\n",
            "Epoch [2/3] Loss: 659.347 | Train Acc: 26.51%\n",
            "Epoch [2/3] Loss: 663.090 | Train Acc: 26.56%\n",
            "Epoch [2/3] Loss: 667.075 | Train Acc: 26.61%\n",
            "Epoch [2/3] Loss: 670.889 | Train Acc: 26.58%\n",
            "Epoch [2/3] Loss: 674.697 | Train Acc: 26.57%\n",
            "Epoch [2/3] Loss: 679.053 | Train Acc: 26.49%\n",
            "Epoch [2/3] Loss: 682.816 | Train Acc: 26.46%\n",
            "Epoch [2/3] Loss: 686.799 | Train Acc: 26.47%\n",
            "Epoch [2/3] Loss: 690.985 | Train Acc: 26.39%\n",
            "Epoch [2/3] Loss: 694.694 | Train Acc: 26.42%\n",
            "Epoch [2/3] Loss: 698.422 | Train Acc: 26.46%\n",
            "Epoch [2/3] Loss: 702.122 | Train Acc: 26.47%\n",
            "Epoch [2/3] Loss: 705.992 | Train Acc: 26.52%\n",
            "Epoch [2/3] Loss: 709.548 | Train Acc: 26.56%\n",
            "Epoch [2/3] Loss: 713.149 | Train Acc: 26.62%\n",
            "Epoch [2/3] Loss: 716.992 | Train Acc: 26.62%\n",
            "Epoch [2/3] Loss: 720.762 | Train Acc: 26.64%\n",
            "Epoch [2/3] Loss: 724.588 | Train Acc: 26.63%\n",
            "Epoch [2/3] Loss: 728.712 | Train Acc: 26.57%\n",
            "Epoch [2/3] Loss: 732.745 | Train Acc: 26.58%\n",
            "Epoch [2/3] Loss: 735.928 | Train Acc: 26.72%\n",
            "Epoch [2/3] Loss: 739.906 | Train Acc: 26.73%\n",
            "Epoch [2/3] Loss: 743.650 | Train Acc: 26.72%\n",
            "Epoch [2/3] Loss: 747.370 | Train Acc: 26.76%\n",
            "Epoch [2/3] Loss: 750.903 | Train Acc: 26.82%\n",
            "Epoch [2/3] Loss: 754.584 | Train Acc: 26.88%\n",
            "Epoch [2/3] Loss: 758.447 | Train Acc: 26.88%\n",
            "Epoch [2/3] Loss: 762.069 | Train Acc: 26.92%\n",
            "Epoch [2/3] Loss: 765.760 | Train Acc: 26.96%\n",
            "Epoch [2/3] Loss: 769.626 | Train Acc: 26.94%\n",
            "Epoch [2/3] Loss: 773.131 | Train Acc: 26.98%\n",
            "Epoch [2/3] Loss: 777.137 | Train Acc: 26.97%\n",
            "Epoch [2/3] Loss: 780.557 | Train Acc: 27.02%\n",
            "Epoch [2/3] Loss: 784.219 | Train Acc: 27.04%\n",
            "Epoch [2/3] Loss: 788.210 | Train Acc: 27.05%\n",
            "Epoch [2/3] Loss: 791.812 | Train Acc: 27.08%\n",
            "Epoch [2/3] Loss: 795.735 | Train Acc: 27.09%\n",
            "Epoch [2/3] Loss: 799.498 | Train Acc: 27.09%\n",
            "Epoch [2/3] Loss: 803.130 | Train Acc: 27.15%\n",
            "Epoch [2/3] Loss: 807.229 | Train Acc: 27.06%\n",
            "Epoch [2/3] Loss: 810.813 | Train Acc: 27.12%\n",
            "Epoch [2/3] Loss: 814.495 | Train Acc: 27.18%\n",
            "Epoch [2/3] Loss: 817.694 | Train Acc: 27.24%\n",
            "Epoch [2/3] Loss: 821.019 | Train Acc: 27.32%\n",
            "Epoch [2/3] Loss: 824.494 | Train Acc: 27.34%\n",
            "Epoch [2/3] Loss: 828.351 | Train Acc: 27.31%\n",
            "Epoch [2/3] Loss: 832.308 | Train Acc: 27.29%\n",
            "Epoch [2/3] Loss: 835.899 | Train Acc: 27.35%\n",
            "Epoch [2/3] Loss: 839.283 | Train Acc: 27.43%\n",
            "Epoch [2/3] Loss: 843.211 | Train Acc: 27.42%\n",
            "Epoch [2/3] Loss: 847.294 | Train Acc: 27.41%\n",
            "Epoch [2/3] Loss: 850.930 | Train Acc: 27.47%\n",
            "Epoch [2/3] Loss: 854.562 | Train Acc: 27.50%\n",
            "Epoch [2/3] Loss: 858.688 | Train Acc: 27.45%\n",
            "Epoch [2/3] Loss: 862.276 | Train Acc: 27.48%\n",
            "Epoch [2/3] Loss: 865.852 | Train Acc: 27.54%\n",
            "Epoch [2/3] Loss: 869.490 | Train Acc: 27.55%\n",
            "Epoch [2/3] Loss: 873.171 | Train Acc: 27.54%\n",
            "Epoch [2/3] Loss: 877.010 | Train Acc: 27.47%\n",
            "Epoch [2/3] Loss: 880.933 | Train Acc: 27.46%\n",
            "Epoch [2/3] Loss: 884.606 | Train Acc: 27.48%\n",
            "Epoch [2/3] Loss: 888.629 | Train Acc: 27.48%\n",
            "Epoch [2/3] Loss: 892.437 | Train Acc: 27.50%\n",
            "Epoch [2/3] Loss: 896.427 | Train Acc: 27.49%\n",
            "Epoch [2/3] Loss: 899.948 | Train Acc: 27.51%\n",
            "Epoch [2/3] Loss: 903.738 | Train Acc: 27.55%\n",
            "Epoch [2/3] Loss: 907.008 | Train Acc: 27.63%\n",
            "Epoch [2/3] Loss: 910.785 | Train Acc: 27.64%\n",
            "Epoch [2/3] Loss: 914.281 | Train Acc: 27.68%\n",
            "Epoch [2/3] Loss: 918.098 | Train Acc: 27.68%\n",
            "Epoch [2/3] Loss: 921.790 | Train Acc: 27.68%\n",
            "Epoch [2/3] Loss: 925.529 | Train Acc: 27.70%\n",
            "Epoch [2/3] Loss: 929.058 | Train Acc: 27.77%\n",
            "Epoch [2/3] Loss: 933.050 | Train Acc: 27.71%\n",
            "Epoch [2/3] Loss: 937.027 | Train Acc: 27.74%\n",
            "Epoch [2/3] Loss: 940.768 | Train Acc: 27.74%\n",
            "Epoch [2/3] Loss: 944.120 | Train Acc: 27.79%\n",
            "Epoch [2/3] Loss: 947.927 | Train Acc: 27.79%\n",
            "Epoch [2/3] Loss: 951.776 | Train Acc: 27.83%\n",
            "Epoch [2/3] Loss: 955.754 | Train Acc: 27.84%\n",
            "Epoch [2/3] Loss: 959.396 | Train Acc: 27.85%\n",
            "Epoch [2/3] Loss: 962.814 | Train Acc: 27.96%\n",
            "Epoch [2/3] Loss: 967.001 | Train Acc: 27.93%\n",
            "Epoch [2/3] Loss: 970.393 | Train Acc: 28.00%\n",
            "Epoch [2/3] Loss: 974.015 | Train Acc: 28.02%\n",
            "Epoch [2/3] Loss: 977.617 | Train Acc: 28.05%\n",
            "Epoch [2/3] Loss: 981.102 | Train Acc: 28.15%\n",
            "Epoch [2/3] Loss: 984.622 | Train Acc: 28.18%\n",
            "Epoch [2/3] Loss: 988.701 | Train Acc: 28.17%\n",
            "Epoch [2/3] Loss: 991.999 | Train Acc: 28.25%\n",
            "Epoch [2/3] Loss: 995.368 | Train Acc: 28.29%\n",
            "Epoch [2/3] Loss: 999.195 | Train Acc: 28.27%\n",
            "Epoch [2/3] Loss: 1002.423 | Train Acc: 28.37%\n",
            "Epoch [2/3] Loss: 1005.669 | Train Acc: 28.44%\n",
            "Epoch [2/3] Loss: 1009.607 | Train Acc: 28.43%\n",
            "Epoch [2/3] Loss: 1013.283 | Train Acc: 28.44%\n",
            "Epoch [2/3] Loss: 1016.783 | Train Acc: 28.45%\n",
            "Epoch [2/3] Loss: 1020.342 | Train Acc: 28.47%\n",
            "Epoch [2/3] Loss: 1023.813 | Train Acc: 28.50%\n",
            "Epoch [2/3] Loss: 1027.168 | Train Acc: 28.53%\n",
            "Epoch [2/3] Loss: 1030.884 | Train Acc: 28.53%\n",
            "Epoch [2/3] Loss: 1034.541 | Train Acc: 28.56%\n",
            "Epoch [2/3] Loss: 1038.005 | Train Acc: 28.57%\n",
            "Epoch [2/3] Loss: 1041.833 | Train Acc: 28.57%\n",
            "Epoch [2/3] Loss: 1045.179 | Train Acc: 28.64%\n",
            "Epoch [2/3] Loss: 1049.007 | Train Acc: 28.63%\n",
            "Epoch [2/3] Loss: 1052.327 | Train Acc: 28.69%\n",
            "Epoch [2/3] Loss: 1056.118 | Train Acc: 28.70%\n",
            "Epoch [2/3] Loss: 1059.526 | Train Acc: 28.74%\n",
            "Epoch [2/3] Loss: 1063.158 | Train Acc: 28.80%\n",
            "Epoch [2/3] Loss: 1066.564 | Train Acc: 28.86%\n",
            "Epoch [2/3] Loss: 1069.910 | Train Acc: 28.90%\n",
            "Epoch [2/3] Loss: 1073.161 | Train Acc: 28.93%\n",
            "Epoch [2/3] Loss: 1076.367 | Train Acc: 28.99%\n",
            "Epoch [2/3] Loss: 1079.832 | Train Acc: 29.03%\n",
            "Epoch [2/3] Loss: 1083.162 | Train Acc: 29.05%\n",
            "Epoch [2/3] Loss: 1086.353 | Train Acc: 29.09%\n",
            "Epoch [2/3] Loss: 1089.992 | Train Acc: 29.11%\n",
            "Epoch [2/3] Loss: 1093.471 | Train Acc: 29.14%\n",
            "Epoch [2/3] Loss: 1096.912 | Train Acc: 29.14%\n",
            "Epoch [2/3] Loss: 1100.225 | Train Acc: 29.19%\n",
            "Epoch [2/3] Loss: 1104.106 | Train Acc: 29.18%\n",
            "Epoch [2/3] Loss: 1107.676 | Train Acc: 29.21%\n",
            "Epoch [2/3] Loss: 1111.582 | Train Acc: 29.17%\n",
            "Epoch [2/3] Loss: 1115.029 | Train Acc: 29.20%\n",
            "Epoch [2/3] Loss: 1118.491 | Train Acc: 29.24%\n",
            "Epoch [2/3] Loss: 1122.040 | Train Acc: 29.25%\n",
            "Epoch [2/3] Loss: 1125.758 | Train Acc: 29.28%\n",
            "Epoch [2/3] Loss: 1129.283 | Train Acc: 29.30%\n",
            "Epoch [2/3] Loss: 1132.695 | Train Acc: 29.33%\n",
            "Epoch [2/3] Loss: 1136.032 | Train Acc: 29.36%\n",
            "Epoch [2/3] Loss: 1139.408 | Train Acc: 29.36%\n",
            "Epoch [2/3] Loss: 1143.074 | Train Acc: 29.35%\n",
            "Epoch [2/3] Loss: 1146.671 | Train Acc: 29.37%\n",
            "Epoch [2/3] Loss: 1150.422 | Train Acc: 29.40%\n",
            "Epoch [2/3] Loss: 1153.953 | Train Acc: 29.36%\n",
            "Epoch [2/3] Loss: 1157.502 | Train Acc: 29.39%\n",
            "Epoch [2/3] Loss: 1160.958 | Train Acc: 29.42%\n",
            "Epoch [2/3] Loss: 1164.817 | Train Acc: 29.40%\n",
            "Epoch [2/3] Loss: 1168.548 | Train Acc: 29.41%\n",
            "Epoch [2/3] Loss: 1172.024 | Train Acc: 29.43%\n",
            "Epoch [2/3] Loss: 1175.588 | Train Acc: 29.43%\n",
            "Epoch [3/3] Loss: 2.853 | Train Acc: 59.38%\n",
            "Epoch [3/3] Loss: 5.936 | Train Acc: 53.12%\n",
            "Epoch [3/3] Loss: 9.326 | Train Acc: 52.08%\n",
            "Epoch [3/3] Loss: 12.334 | Train Acc: 49.22%\n",
            "Epoch [3/3] Loss: 15.401 | Train Acc: 50.62%\n",
            "Epoch [3/3] Loss: 18.227 | Train Acc: 53.12%\n",
            "Epoch [3/3] Loss: 21.157 | Train Acc: 52.23%\n",
            "Epoch [3/3] Loss: 23.803 | Train Acc: 52.34%\n",
            "Epoch [3/3] Loss: 26.750 | Train Acc: 52.43%\n",
            "Epoch [3/3] Loss: 29.753 | Train Acc: 52.19%\n",
            "Epoch [3/3] Loss: 32.833 | Train Acc: 51.99%\n",
            "Epoch [3/3] Loss: 35.620 | Train Acc: 52.60%\n",
            "Epoch [3/3] Loss: 38.724 | Train Acc: 52.64%\n",
            "Epoch [3/3] Loss: 42.296 | Train Acc: 50.00%\n",
            "Epoch [3/3] Loss: 45.503 | Train Acc: 50.00%\n",
            "Epoch [3/3] Loss: 48.517 | Train Acc: 50.00%\n",
            "Epoch [3/3] Loss: 51.749 | Train Acc: 50.00%\n",
            "Epoch [3/3] Loss: 54.735 | Train Acc: 50.00%\n",
            "Epoch [3/3] Loss: 57.802 | Train Acc: 49.51%\n",
            "Epoch [3/3] Loss: 60.653 | Train Acc: 49.69%\n",
            "Epoch [3/3] Loss: 63.844 | Train Acc: 49.40%\n",
            "Epoch [3/3] Loss: 67.367 | Train Acc: 48.72%\n",
            "Epoch [3/3] Loss: 70.285 | Train Acc: 48.91%\n",
            "Epoch [3/3] Loss: 73.176 | Train Acc: 48.83%\n",
            "Epoch [3/3] Loss: 76.340 | Train Acc: 48.75%\n",
            "Epoch [3/3] Loss: 79.628 | Train Acc: 48.56%\n",
            "Epoch [3/3] Loss: 82.992 | Train Acc: 48.03%\n",
            "Epoch [3/3] Loss: 86.228 | Train Acc: 47.77%\n",
            "Epoch [3/3] Loss: 89.739 | Train Acc: 47.31%\n",
            "Epoch [3/3] Loss: 92.604 | Train Acc: 47.60%\n",
            "Epoch [3/3] Loss: 95.783 | Train Acc: 47.58%\n",
            "Epoch [3/3] Loss: 98.483 | Train Acc: 47.75%\n",
            "Epoch [3/3] Loss: 101.739 | Train Acc: 47.44%\n",
            "Epoch [3/3] Loss: 104.756 | Train Acc: 47.70%\n",
            "Epoch [3/3] Loss: 107.979 | Train Acc: 47.32%\n",
            "Epoch [3/3] Loss: 111.228 | Train Acc: 47.14%\n",
            "Epoch [3/3] Loss: 114.580 | Train Acc: 46.88%\n",
            "Epoch [3/3] Loss: 117.740 | Train Acc: 46.96%\n",
            "Epoch [3/3] Loss: 120.913 | Train Acc: 46.88%\n",
            "Epoch [3/3] Loss: 124.028 | Train Acc: 47.11%\n",
            "Epoch [3/3] Loss: 127.008 | Train Acc: 47.18%\n",
            "Epoch [3/3] Loss: 130.006 | Train Acc: 47.32%\n",
            "Epoch [3/3] Loss: 133.255 | Train Acc: 47.02%\n",
            "Epoch [3/3] Loss: 136.488 | Train Acc: 46.88%\n",
            "Epoch [3/3] Loss: 139.852 | Train Acc: 46.46%\n",
            "Epoch [3/3] Loss: 142.917 | Train Acc: 46.47%\n",
            "Epoch [3/3] Loss: 146.222 | Train Acc: 46.21%\n",
            "Epoch [3/3] Loss: 149.834 | Train Acc: 45.83%\n",
            "Epoch [3/3] Loss: 152.988 | Train Acc: 45.92%\n",
            "Epoch [3/3] Loss: 155.874 | Train Acc: 46.06%\n",
            "Epoch [3/3] Loss: 159.386 | Train Acc: 45.89%\n",
            "Epoch [3/3] Loss: 162.498 | Train Acc: 45.91%\n",
            "Epoch [3/3] Loss: 165.600 | Train Acc: 46.23%\n",
            "Epoch [3/3] Loss: 168.958 | Train Acc: 46.12%\n",
            "Epoch [3/3] Loss: 171.828 | Train Acc: 46.14%\n",
            "Epoch [3/3] Loss: 174.765 | Train Acc: 46.37%\n",
            "Epoch [3/3] Loss: 177.954 | Train Acc: 46.16%\n",
            "Epoch [3/3] Loss: 181.017 | Train Acc: 46.28%\n",
            "Epoch [3/3] Loss: 184.139 | Train Acc: 46.35%\n",
            "Epoch [3/3] Loss: 187.543 | Train Acc: 46.25%\n",
            "Epoch [3/3] Loss: 190.632 | Train Acc: 46.26%\n",
            "Epoch [3/3] Loss: 193.715 | Train Acc: 46.27%\n",
            "Epoch [3/3] Loss: 196.739 | Train Acc: 46.23%\n",
            "Epoch [3/3] Loss: 199.716 | Train Acc: 46.34%\n",
            "Epoch [3/3] Loss: 202.640 | Train Acc: 46.44%\n",
            "Epoch [3/3] Loss: 205.691 | Train Acc: 46.50%\n",
            "Epoch [3/3] Loss: 209.153 | Train Acc: 46.41%\n",
            "Epoch [3/3] Loss: 212.151 | Train Acc: 46.51%\n",
            "Epoch [3/3] Loss: 215.152 | Train Acc: 46.60%\n",
            "Epoch [3/3] Loss: 217.842 | Train Acc: 46.74%\n",
            "Epoch [3/3] Loss: 220.787 | Train Acc: 46.83%\n",
            "Epoch [3/3] Loss: 223.704 | Train Acc: 46.83%\n",
            "Epoch [3/3] Loss: 226.562 | Train Acc: 47.05%\n",
            "Epoch [3/3] Loss: 229.091 | Train Acc: 47.30%\n",
            "Epoch [3/3] Loss: 232.073 | Train Acc: 47.25%\n",
            "Epoch [3/3] Loss: 235.011 | Train Acc: 47.29%\n",
            "Epoch [3/3] Loss: 237.733 | Train Acc: 47.36%\n",
            "Epoch [3/3] Loss: 240.639 | Train Acc: 47.36%\n",
            "Epoch [3/3] Loss: 243.920 | Train Acc: 47.31%\n",
            "Epoch [3/3] Loss: 247.315 | Train Acc: 47.23%\n",
            "Epoch [3/3] Loss: 249.966 | Train Acc: 47.45%\n",
            "Epoch [3/3] Loss: 253.265 | Train Acc: 47.41%\n",
            "Epoch [3/3] Loss: 256.095 | Train Acc: 47.52%\n",
            "Epoch [3/3] Loss: 259.256 | Train Acc: 47.43%\n",
            "Epoch [3/3] Loss: 262.673 | Train Acc: 47.35%\n",
            "Epoch [3/3] Loss: 265.580 | Train Acc: 47.46%\n",
            "Epoch [3/3] Loss: 268.426 | Train Acc: 47.52%\n",
            "Epoch [3/3] Loss: 271.676 | Train Acc: 47.55%\n",
            "Epoch [3/3] Loss: 274.433 | Train Acc: 47.72%\n",
            "Epoch [3/3] Loss: 277.740 | Train Acc: 47.64%\n",
            "Epoch [3/3] Loss: 280.425 | Train Acc: 47.60%\n",
            "Epoch [3/3] Loss: 283.374 | Train Acc: 47.59%\n",
            "Epoch [3/3] Loss: 286.219 | Train Acc: 47.68%\n",
            "Epoch [3/3] Loss: 289.224 | Train Acc: 47.71%\n",
            "Epoch [3/3] Loss: 291.955 | Train Acc: 47.80%\n",
            "Epoch [3/3] Loss: 295.204 | Train Acc: 47.75%\n",
            "Epoch [3/3] Loss: 298.501 | Train Acc: 47.58%\n",
            "Epoch [3/3] Loss: 301.381 | Train Acc: 47.67%\n",
            "Epoch [3/3] Loss: 304.524 | Train Acc: 47.54%\n",
            "Epoch [3/3] Loss: 306.865 | Train Acc: 47.78%\n",
            "Epoch [3/3] Loss: 309.764 | Train Acc: 47.87%\n",
            "Epoch [3/3] Loss: 312.422 | Train Acc: 47.98%\n",
            "Epoch [3/3] Loss: 315.401 | Train Acc: 48.03%\n",
            "Epoch [3/3] Loss: 318.052 | Train Acc: 48.26%\n",
            "Epoch [3/3] Loss: 321.216 | Train Acc: 48.21%\n",
            "Epoch [3/3] Loss: 323.962 | Train Acc: 48.32%\n",
            "Epoch [3/3] Loss: 327.252 | Train Acc: 48.25%\n",
            "Epoch [3/3] Loss: 330.110 | Train Acc: 48.21%\n",
            "Epoch [3/3] Loss: 333.007 | Train Acc: 48.28%\n",
            "Epoch [3/3] Loss: 335.781 | Train Acc: 48.38%\n",
            "Epoch [3/3] Loss: 338.988 | Train Acc: 48.34%\n",
            "Epoch [3/3] Loss: 341.740 | Train Acc: 48.35%\n",
            "Epoch [3/3] Loss: 344.439 | Train Acc: 48.45%\n",
            "Epoch [3/3] Loss: 347.506 | Train Acc: 48.46%\n",
            "Epoch [3/3] Loss: 350.385 | Train Acc: 48.51%\n",
            "Epoch [3/3] Loss: 353.128 | Train Acc: 48.55%\n",
            "Epoch [3/3] Loss: 356.122 | Train Acc: 48.58%\n",
            "Epoch [3/3] Loss: 358.916 | Train Acc: 48.65%\n",
            "Epoch [3/3] Loss: 362.014 | Train Acc: 48.58%\n",
            "Epoch [3/3] Loss: 365.147 | Train Acc: 48.59%\n",
            "Epoch [3/3] Loss: 368.335 | Train Acc: 48.61%\n",
            "Epoch [3/3] Loss: 371.307 | Train Acc: 48.67%\n",
            "Epoch [3/3] Loss: 374.572 | Train Acc: 48.58%\n",
            "Epoch [3/3] Loss: 377.590 | Train Acc: 48.59%\n",
            "Epoch [3/3] Loss: 380.656 | Train Acc: 48.65%\n",
            "Epoch [3/3] Loss: 383.477 | Train Acc: 48.56%\n",
            "Epoch [3/3] Loss: 386.363 | Train Acc: 48.55%\n",
            "Epoch [3/3] Loss: 389.355 | Train Acc: 48.46%\n",
            "Epoch [3/3] Loss: 392.079 | Train Acc: 48.45%\n",
            "Epoch [3/3] Loss: 395.017 | Train Acc: 48.49%\n",
            "Epoch [3/3] Loss: 397.723 | Train Acc: 48.45%\n",
            "Epoch [3/3] Loss: 400.977 | Train Acc: 48.39%\n",
            "Epoch [3/3] Loss: 403.759 | Train Acc: 48.40%\n",
            "Epoch [3/3] Loss: 406.793 | Train Acc: 48.44%\n",
            "Epoch [3/3] Loss: 409.726 | Train Acc: 48.40%\n",
            "Epoch [3/3] Loss: 412.782 | Train Acc: 48.37%\n",
            "Epoch [3/3] Loss: 415.220 | Train Acc: 48.49%\n",
            "Epoch [3/3] Loss: 418.350 | Train Acc: 48.48%\n",
            "Epoch [3/3] Loss: 421.076 | Train Acc: 48.58%\n",
            "Epoch [3/3] Loss: 423.615 | Train Acc: 48.62%\n",
            "Epoch [3/3] Loss: 426.338 | Train Acc: 48.67%\n",
            "Epoch [3/3] Loss: 429.227 | Train Acc: 48.68%\n",
            "Epoch [3/3] Loss: 431.967 | Train Acc: 48.75%\n",
            "Epoch [3/3] Loss: 435.122 | Train Acc: 48.68%\n",
            "Epoch [3/3] Loss: 437.832 | Train Acc: 48.71%\n",
            "Epoch [3/3] Loss: 440.879 | Train Acc: 48.67%\n",
            "Epoch [3/3] Loss: 443.722 | Train Acc: 48.66%\n",
            "Epoch [3/3] Loss: 446.460 | Train Acc: 48.63%\n",
            "Epoch [3/3] Loss: 449.143 | Train Acc: 48.74%\n",
            "Epoch [3/3] Loss: 451.873 | Train Acc: 48.77%\n",
            "Epoch [3/3] Loss: 454.950 | Train Acc: 48.74%\n",
            "Epoch [3/3] Loss: 457.826 | Train Acc: 48.83%\n",
            "Epoch [3/3] Loss: 460.735 | Train Acc: 48.86%\n",
            "Epoch [3/3] Loss: 463.531 | Train Acc: 48.88%\n",
            "Epoch [3/3] Loss: 466.383 | Train Acc: 48.97%\n",
            "Epoch [3/3] Loss: 469.431 | Train Acc: 48.90%\n",
            "Epoch [3/3] Loss: 472.720 | Train Acc: 48.81%\n",
            "Epoch [3/3] Loss: 475.613 | Train Acc: 48.79%\n",
            "Epoch [3/3] Loss: 478.802 | Train Acc: 48.70%\n",
            "Epoch [3/3] Loss: 481.475 | Train Acc: 48.75%\n",
            "Epoch [3/3] Loss: 484.107 | Train Acc: 48.82%\n",
            "Epoch [3/3] Loss: 487.163 | Train Acc: 48.75%\n",
            "Epoch [3/3] Loss: 490.229 | Train Acc: 48.70%\n",
            "Epoch [3/3] Loss: 492.650 | Train Acc: 48.80%\n",
            "Epoch [3/3] Loss: 495.411 | Train Acc: 48.84%\n",
            "Epoch [3/3] Loss: 498.376 | Train Acc: 48.80%\n",
            "Epoch [3/3] Loss: 501.597 | Train Acc: 48.78%\n",
            "Epoch [3/3] Loss: 504.294 | Train Acc: 48.75%\n",
            "Epoch [3/3] Loss: 507.074 | Train Acc: 48.74%\n",
            "Epoch [3/3] Loss: 509.999 | Train Acc: 48.73%\n",
            "Epoch [3/3] Loss: 512.680 | Train Acc: 48.79%\n",
            "Epoch [3/3] Loss: 515.420 | Train Acc: 48.84%\n",
            "Epoch [3/3] Loss: 518.511 | Train Acc: 48.83%\n",
            "Epoch [3/3] Loss: 521.165 | Train Acc: 48.85%\n",
            "Epoch [3/3] Loss: 523.244 | Train Acc: 48.98%\n",
            "Epoch [3/3] Loss: 526.268 | Train Acc: 48.97%\n",
            "Epoch [3/3] Loss: 528.919 | Train Acc: 48.96%\n",
            "Epoch [3/3] Loss: 532.214 | Train Acc: 48.91%\n",
            "Epoch [3/3] Loss: 534.403 | Train Acc: 49.02%\n",
            "Epoch [3/3] Loss: 537.521 | Train Acc: 48.99%\n",
            "Epoch [3/3] Loss: 540.541 | Train Acc: 48.96%\n",
            "Epoch [3/3] Loss: 543.620 | Train Acc: 48.97%\n",
            "Epoch [3/3] Loss: 546.374 | Train Acc: 48.94%\n",
            "Epoch [3/3] Loss: 549.117 | Train Acc: 48.93%\n",
            "Epoch [3/3] Loss: 551.690 | Train Acc: 48.97%\n",
            "Epoch [3/3] Loss: 554.308 | Train Acc: 49.03%\n",
            "Epoch [3/3] Loss: 557.223 | Train Acc: 49.03%\n",
            "Epoch [3/3] Loss: 560.308 | Train Acc: 48.94%\n",
            "Epoch [3/3] Loss: 563.229 | Train Acc: 48.96%\n",
            "Epoch [3/3] Loss: 566.254 | Train Acc: 48.96%\n",
            "Epoch [3/3] Loss: 569.725 | Train Acc: 48.89%\n",
            "Epoch [3/3] Loss: 572.349 | Train Acc: 48.96%\n",
            "Epoch [3/3] Loss: 575.012 | Train Acc: 49.01%\n",
            "Epoch [3/3] Loss: 578.050 | Train Acc: 48.94%\n",
            "Epoch [3/3] Loss: 580.926 | Train Acc: 48.97%\n",
            "Epoch [3/3] Loss: 583.531 | Train Acc: 49.00%\n",
            "Epoch [3/3] Loss: 586.276 | Train Acc: 49.02%\n",
            "Epoch [3/3] Loss: 589.006 | Train Acc: 49.02%\n",
            "Epoch [3/3] Loss: 591.539 | Train Acc: 49.09%\n",
            "Epoch [3/3] Loss: 594.597 | Train Acc: 49.08%\n",
            "Epoch [3/3] Loss: 597.318 | Train Acc: 49.10%\n",
            "Epoch [3/3] Loss: 600.113 | Train Acc: 49.07%\n",
            "Epoch [3/3] Loss: 603.058 | Train Acc: 49.03%\n",
            "Epoch [3/3] Loss: 605.473 | Train Acc: 49.10%\n",
            "Epoch [3/3] Loss: 608.574 | Train Acc: 49.09%\n",
            "Epoch [3/3] Loss: 611.812 | Train Acc: 49.03%\n",
            "Epoch [3/3] Loss: 614.663 | Train Acc: 49.03%\n",
            "Epoch [3/3] Loss: 617.114 | Train Acc: 49.11%\n",
            "Epoch [3/3] Loss: 620.072 | Train Acc: 49.13%\n",
            "Epoch [3/3] Loss: 622.705 | Train Acc: 49.18%\n",
            "Epoch [3/3] Loss: 625.164 | Train Acc: 49.26%\n",
            "Epoch [3/3] Loss: 627.552 | Train Acc: 49.34%\n",
            "Epoch [3/3] Loss: 630.617 | Train Acc: 49.25%\n",
            "Epoch [3/3] Loss: 633.026 | Train Acc: 49.33%\n",
            "Epoch [3/3] Loss: 635.877 | Train Acc: 49.35%\n",
            "Epoch [3/3] Loss: 638.832 | Train Acc: 49.31%\n",
            "Epoch [3/3] Loss: 641.971 | Train Acc: 49.24%\n",
            "Epoch [3/3] Loss: 644.647 | Train Acc: 49.27%\n",
            "Epoch [3/3] Loss: 647.089 | Train Acc: 49.29%\n",
            "Epoch [3/3] Loss: 649.639 | Train Acc: 49.32%\n",
            "Epoch [3/3] Loss: 652.271 | Train Acc: 49.35%\n",
            "Epoch [3/3] Loss: 654.932 | Train Acc: 49.34%\n",
            "Epoch [3/3] Loss: 657.658 | Train Acc: 49.37%\n",
            "Epoch [3/3] Loss: 660.012 | Train Acc: 49.40%\n",
            "Epoch [3/3] Loss: 662.829 | Train Acc: 49.44%\n",
            "Epoch [3/3] Loss: 665.460 | Train Acc: 49.47%\n",
            "Epoch [3/3] Loss: 668.232 | Train Acc: 49.46%\n",
            "Epoch [3/3] Loss: 671.643 | Train Acc: 49.40%\n",
            "Epoch [3/3] Loss: 674.787 | Train Acc: 49.39%\n",
            "Epoch [3/3] Loss: 677.480 | Train Acc: 49.42%\n",
            "Epoch [3/3] Loss: 680.093 | Train Acc: 49.49%\n",
            "Epoch [3/3] Loss: 682.636 | Train Acc: 49.53%\n",
            "Epoch [3/3] Loss: 685.132 | Train Acc: 49.57%\n",
            "Epoch [3/3] Loss: 687.798 | Train Acc: 49.61%\n",
            "Epoch [3/3] Loss: 690.395 | Train Acc: 49.65%\n",
            "Epoch [3/3] Loss: 693.379 | Train Acc: 49.60%\n",
            "Epoch [3/3] Loss: 696.239 | Train Acc: 49.62%\n",
            "Epoch [3/3] Loss: 698.690 | Train Acc: 49.65%\n",
            "Epoch [3/3] Loss: 701.634 | Train Acc: 49.65%\n",
            "Epoch [3/3] Loss: 704.402 | Train Acc: 49.66%\n",
            "Epoch [3/3] Loss: 707.349 | Train Acc: 49.65%\n",
            "Epoch [3/3] Loss: 710.024 | Train Acc: 49.68%\n",
            "Epoch [3/3] Loss: 712.752 | Train Acc: 49.67%\n",
            "Epoch [3/3] Loss: 715.471 | Train Acc: 49.71%\n",
            "Epoch [3/3] Loss: 718.655 | Train Acc: 49.67%\n",
            "Epoch [3/3] Loss: 721.517 | Train Acc: 49.64%\n",
            "Epoch [3/3] Loss: 724.622 | Train Acc: 49.62%\n",
            "Epoch [3/3] Loss: 727.513 | Train Acc: 49.63%\n",
            "Epoch [3/3] Loss: 730.439 | Train Acc: 49.62%\n",
            "Epoch [3/3] Loss: 733.140 | Train Acc: 49.67%\n",
            "Epoch [3/3] Loss: 735.549 | Train Acc: 49.70%\n",
            "Epoch [3/3] Loss: 738.527 | Train Acc: 49.70%\n",
            "Epoch [3/3] Loss: 741.488 | Train Acc: 49.70%\n",
            "Epoch [3/3] Loss: 743.845 | Train Acc: 49.75%\n",
            "Epoch [3/3] Loss: 746.567 | Train Acc: 49.74%\n",
            "Epoch [3/3] Loss: 748.978 | Train Acc: 49.79%\n",
            "Epoch [3/3] Loss: 752.017 | Train Acc: 49.72%\n",
            "Epoch [3/3] Loss: 754.625 | Train Acc: 49.75%\n",
            "Epoch [3/3] Loss: 757.290 | Train Acc: 49.79%\n",
            "Epoch [3/3] Loss: 759.800 | Train Acc: 49.84%\n",
            "Epoch [3/3] Loss: 762.731 | Train Acc: 49.83%\n",
            "Epoch [3/3] Loss: 765.365 | Train Acc: 49.81%\n",
            "Epoch [3/3] Loss: 767.938 | Train Acc: 49.82%\n",
            "Epoch [3/3] Loss: 770.728 | Train Acc: 49.76%\n",
            "Epoch [3/3] Loss: 773.597 | Train Acc: 49.79%\n",
            "Epoch [3/3] Loss: 776.092 | Train Acc: 49.86%\n",
            "Epoch [3/3] Loss: 778.737 | Train Acc: 49.89%\n",
            "Epoch [3/3] Loss: 781.505 | Train Acc: 49.88%\n",
            "Epoch [3/3] Loss: 784.295 | Train Acc: 49.88%\n",
            "Epoch [3/3] Loss: 787.039 | Train Acc: 49.88%\n",
            "Epoch [3/3] Loss: 790.332 | Train Acc: 49.83%\n",
            "Epoch [3/3] Loss: 792.933 | Train Acc: 49.87%\n",
            "Epoch [3/3] Loss: 795.966 | Train Acc: 49.84%\n",
            "Epoch [3/3] Loss: 798.422 | Train Acc: 49.89%\n",
            "Epoch [3/3] Loss: 801.141 | Train Acc: 49.91%\n",
            "Epoch [3/3] Loss: 803.471 | Train Acc: 49.95%\n",
            "Epoch [3/3] Loss: 805.968 | Train Acc: 50.06%\n",
            "Epoch [3/3] Loss: 808.633 | Train Acc: 50.06%\n",
            "Epoch [3/3] Loss: 811.400 | Train Acc: 50.07%\n",
            "Epoch [3/3] Loss: 813.950 | Train Acc: 50.09%\n",
            "Epoch [3/3] Loss: 816.599 | Train Acc: 50.09%\n",
            "Epoch [3/3] Loss: 819.525 | Train Acc: 50.09%\n",
            "Epoch [3/3] Loss: 822.259 | Train Acc: 50.04%\n",
            "Epoch [3/3] Loss: 824.726 | Train Acc: 50.10%\n",
            "Epoch [3/3] Loss: 827.494 | Train Acc: 50.10%\n",
            "Epoch [3/3] Loss: 829.818 | Train Acc: 50.14%\n",
            "Epoch [3/3] Loss: 832.355 | Train Acc: 50.14%\n",
            "Epoch [3/3] Loss: 834.650 | Train Acc: 50.23%\n",
            "Epoch [3/3] Loss: 837.378 | Train Acc: 50.26%\n",
            "Epoch [3/3] Loss: 839.802 | Train Acc: 50.27%\n",
            "Epoch [3/3] Loss: 842.507 | Train Acc: 50.30%\n",
            "Epoch [3/3] Loss: 845.287 | Train Acc: 50.31%\n",
            "Epoch [3/3] Loss: 847.635 | Train Acc: 50.35%\n",
            "Epoch [3/3] Loss: 850.122 | Train Acc: 50.38%\n",
            "Epoch [3/3] Loss: 852.657 | Train Acc: 50.42%\n",
            "Epoch [3/3] Loss: 855.243 | Train Acc: 50.43%\n",
            "Epoch [3/3] Loss: 857.829 | Train Acc: 50.45%\n",
            "Epoch [3/3] Loss: 860.112 | Train Acc: 50.49%\n",
            "Epoch [3/3] Loss: 862.440 | Train Acc: 50.53%\n",
            "Epoch [3/3] Loss: 865.580 | Train Acc: 50.49%\n",
            "Epoch [3/3] Loss: 868.643 | Train Acc: 50.45%\n",
            "Epoch [3/3] Loss: 871.384 | Train Acc: 50.46%\n",
            "Epoch [3/3] Loss: 874.020 | Train Acc: 50.45%\n",
            "Epoch [3/3] Loss: 876.562 | Train Acc: 50.46%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **image to embeddings**"
      ],
      "metadata": {
        "id": "vcyKfroFosQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet"
      ],
      "metadata": {
        "id": "ypM6xKQQozbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        output = model(images)\n",
        "        _, preds = torch.max(output, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "val_acc = 100 * correct / total\n",
        "print(\"Validation Accuracy:\", val_acc)\n"
      ],
      "metadata": {
        "id": "FMGe4orqj2rF",
        "outputId": "50aaafb6-f051-44e9-e13d-0be0bebce479",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(\n",
        "    model.state_dict(),\n",
        "    \"/content/drive/MyDrive/face_proj/phase1_casia_magf.pth\"\n",
        ")\n",
        "print(\"Model saved\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoA2YRwTkBSe",
        "outputId": "d260bd3f-0441-4461-e4d0-da6ef5125fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.classes[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA2jWjgKZLQ3",
        "outputId": "65ff73b8-502a-4a56-daed-5534b5647c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['0000045', '0000099', '0000100', '0000102', '0000103', '0000105', '0000107', '0000108', '0000114', '0000117']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "base_model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Remove classifier entirely\n",
        "base_model.fc = nn.Identity()\n",
        "\n",
        "base_model = base_model.to(device)\n",
        "base_model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W410KYz7Zd8d",
        "outputId": "67443db7-ee64-4b8d-9039-0fabab4ebee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Identity()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = []\n",
        "labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, lbls in val_loader:\n",
        "        images = images.to(device)\n",
        "        feats = base_model(images)   # [B, 512]\n",
        "\n",
        "        embeddings.append(feats.cpu())\n",
        "        labels.append(lbls)\n",
        "\n",
        "embeddings = torch.cat(embeddings).numpy()\n",
        "labels = torch.cat(labels).numpy()\n",
        "\n",
        "print(\"Embeddings shape:\", embeddings.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeYQHCi_Zm4L",
        "outputId": "b707a384-018f-425b-f40c-dce1de25cfe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape: (2551, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Map identity â†’ embedding indices\n",
        "id_to_indices = defaultdict(list)\n",
        "for idx, lbl in enumerate(labels):\n",
        "    id_to_indices[lbl].append(idx)\n",
        "\n",
        "pairs = []\n",
        "pair_labels = []\n",
        "\n",
        "# Positive pairs (same identity)\n",
        "for lbl, idxs in id_to_indices.items():\n",
        "    if len(idxs) >= 2:\n",
        "        i1, i2 = random.sample(idxs, 2)\n",
        "        pairs.append((i1, i2))\n",
        "        pair_labels.append(1)\n",
        "\n",
        "# Negative pairs (different identities)\n",
        "all_labels = list(id_to_indices.keys())\n",
        "\n",
        "for _ in range(len(pairs)):\n",
        "    l1, l2 = random.sample(all_labels, 2)\n",
        "    i1 = random.choice(id_to_indices[l1])\n",
        "    i2 = random.choice(id_to_indices[l2])\n",
        "    pairs.append((i1, i2))\n",
        "    pair_labels.append(0)\n",
        "\n",
        "pairs = np.array(pairs)\n",
        "pair_labels = np.array(pair_labels)\n",
        "\n",
        "print(\"Total pairs:\", len(pairs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB_AJMJPaG6C",
        "outputId": "57292700-bab6-4c15-a58e-8191adf6122a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pairs: 140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "scores = []\n",
        "\n",
        "for i1, i2 in pairs:\n",
        "    sim = cosine_similarity(\n",
        "        embeddings[i1].reshape(1, -1),\n",
        "        embeddings[i2].reshape(1, -1)\n",
        "    )[0][0]\n",
        "    scores.append(sim)\n",
        "\n",
        "scores = np.array(scores)\n"
      ],
      "metadata": {
        "id": "RolhjAQgb2vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "same_scores = scores[pair_labels == 1]\n",
        "diff_scores = scores[pair_labels == 0]\n",
        "\n",
        "print(\"Same-person mean similarity:\", same_scores.mean())\n",
        "print(\"Different-person mean similarity:\", diff_scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWyzwcc-dG5v",
        "outputId": "3c11f647-cfbf-48cc-d77d-42a60d6df684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Same-person mean similarity: 0.8047906\n",
            "Different-person mean similarity: 0.7520156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##EER\n",
        "from sklearn.metrics import roc_curve\n",
        "import numpy as np\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(pair_labels, scores, pos_label=1)\n",
        "fnr = 1 - tpr\n",
        "\n",
        "eer_idx = np.nanargmin(np.abs(fpr - fnr))\n",
        "eer = fpr[eer_idx]\n",
        "\n",
        "print(\"EER:\", eer) #shows how many decisions are wrong\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgvDMUN_nrBz",
        "outputId": "9f8ab937-1970-499f-eb52-190169324665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EER: 0.37142857142857144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet+ArcFace Loss function"
      ],
      "metadata": {
        "id": "d9TdSqqSo5Rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install insightface\n"
      ],
      "metadata": {
        "id": "5MIlHvtHcN2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class ArcMarginProduct(nn.Module):\n",
        "    def __init__(self, in_features, out_features, s=30.0, m=0.50):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "        self.cos_m = math.cos(m)\n",
        "        self.sin_m = math.sin(m)\n",
        "        self.th = math.cos(math.pi - m)\n",
        "        self.mm = math.sin(math.pi - m) * m\n",
        "\n",
        "    def forward(self, x, label):\n",
        "        cosine = F.linear(F.normalize(x), F.normalize(self.weight))\n",
        "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
        "\n",
        "        one_hot = torch.zeros(cosine.size(), device=x.device)\n",
        "        one_hot.scatter_(1, label.view(-1, 1), 1.0)\n",
        "\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "P4o_z7SRd1Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "num_classes = len(dataset.classes)\n",
        "\n",
        "backbone = models.resnet18(pretrained=True)\n",
        "backbone.fc = nn.Identity()\n",
        "backbone = backbone.to(device)\n",
        "\n",
        "arcface = ArcMarginProduct(\n",
        "    in_features=512,\n",
        "    out_features=num_classes,\n",
        "    s=30.0,\n",
        "    m=0.5\n",
        ").to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujBXUkEIeAVf",
        "outputId": "20b0a706-0432-429b-8e73-41a15314a74e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(backbone.parameters()) + list(arcface.parameters()),\n",
        "    lr=1e-4\n",
        ")\n"
      ],
      "metadata": {
        "id": "9n69hoSqiBfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(15):   # 3â€“5 epochs is enough for now\n",
        "    backbone.train()\n",
        "    arcface.train()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        embeddings = backbone(images)\n",
        "        logits = arcface(embeddings, labels)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}] Loss: {total_loss:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "lvINgBXbiGFT",
        "outputId": "4fcb7530-4413-4f22-9c81-91cb3aa92a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3066143442.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch [{epoch+1}] Loss: {total_loss:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "backbone.eval()\n",
        "\n",
        "embeddings = []\n",
        "labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, lbls in val_loader:\n",
        "        images = images.to(device)\n",
        "        feats = backbone(images)\n",
        "        embeddings.append(feats.cpu())\n",
        "        labels.append(lbls)\n",
        "\n",
        "embeddings = torch.cat(embeddings).numpy()\n",
        "labels = torch.cat(labels).numpy()\n",
        "\n",
        "from sklearn.preprocessing import normalize\n",
        "embeddings = normalize(embeddings)\n"
      ],
      "metadata": {
        "id": "HqlIEGUFiSxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "id_to_indices = defaultdict(list)\n",
        "for idx, lbl in enumerate(labels):\n",
        "    id_to_indices[lbl].append(idx)\n",
        "\n",
        "pairs = []\n",
        "pair_labels = []   # 1 = same person, 0 = different\n",
        "\n",
        "# Positive (same identity)\n",
        "for lbl, idxs in id_to_indices.items():\n",
        "    if len(idxs) >= 2:\n",
        "        i1, i2 = random.sample(idxs, 2)\n",
        "        pairs.append((i1, i2))\n",
        "        pair_labels.append(1)\n",
        "\n",
        "# Negative (different identity)\n",
        "all_labels = list(id_to_indices.keys())\n",
        "\n",
        "for _ in range(len(pairs)):\n",
        "    l1, l2 = random.sample(all_labels, 2)\n",
        "    i1 = random.choice(id_to_indices[l1])\n",
        "    i2 = random.choice(id_to_indices[l2])\n",
        "    pairs.append((i1, i2))\n",
        "    pair_labels.append(0)\n",
        "\n",
        "pairs = np.array(pairs)\n",
        "pair_labels = np.array(pair_labels)\n",
        "\n",
        "print(\"Total verification pairs:\", len(pairs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Fulftf_i1nq",
        "outputId": "e191d595-a35d-4126-c3d8-ab3d0528c864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total verification pairs: 120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "scores = []\n",
        "\n",
        "for i1, i2 in pairs:\n",
        "    sim = cosine_similarity(\n",
        "        embeddings[i1].reshape(1, -1),\n",
        "        embeddings[i2].reshape(1, -1)\n",
        "    )[0][0]\n",
        "    scores.append(sim)\n",
        "\n",
        "scores = np.array(scores)\n"
      ],
      "metadata": {
        "id": "efnGyEpOiW4D",
        "outputId": "0939cbaa-382e-47e2-822b-24b991fc59e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 32 is out of bounds for dimension 0 with size 32",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-392238356.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     sim = cosine_similarity(\n\u001b[1;32m      7\u001b[0m         \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     )[0][0]\n\u001b[1;32m     10\u001b[0m     \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 32 is out of bounds for dimension 0 with size 32"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(pair_labels, scores, pos_label=1)\n",
        "fnr = 1 - tpr\n"
      ],
      "metadata": {
        "id": "0LpGVnpVi8l3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "eer_index = np.nanargmin(np.abs(fpr - fnr))\n",
        "eer = fpr[eer_index]\n",
        "\n",
        "print(\"EER:\", eer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IgobHadi_Zu",
        "outputId": "4221bbb3-0693-448a-c66f-078c5f28a2da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EER: 0.45714285714285713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet+MagFace"
      ],
      "metadata": {
        "id": "URZ-MyXDmGoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MagFace(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features,\n",
        "        out_features,\n",
        "        s=30.0,\n",
        "        m_min=0.45,\n",
        "        m_max=0.8,\n",
        "        l_a=10,\n",
        "        u_a=110\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s\n",
        "\n",
        "        self.m_min = m_min\n",
        "        self.m_max = m_max\n",
        "        self.l_a = l_a\n",
        "        self.u_a = u_a\n",
        "\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, embeddings, labels):\n",
        "        # embedding norm = quality\n",
        "        norm = torch.norm(embeddings, dim=1, keepdim=True).clamp(self.l_a, self.u_a)\n",
        "\n",
        "        # adaptive margin\n",
        "        margin = (\n",
        "            self.m_min +\n",
        "            (self.m_max - self.m_min) *\n",
        "            (norm - self.l_a) / (self.u_a - self.l_a)\n",
        "        )\n",
        "\n",
        "        cosine = F.linear(F.normalize(embeddings), F.normalize(self.weight))\n",
        "        sine = torch.sqrt(1.0 - cosine ** 2 + 1e-6)\n",
        "        phi = cosine * torch.cos(margin) - sine * torch.sin(margin)\n",
        "\n",
        "        one_hot = torch.zeros_like(cosine)\n",
        "        one_hot.scatter_(1, labels.view(-1, 1), 1.0)\n",
        "\n",
        "        output = one_hot * phi + (1.0 - one_hot) * cosine\n",
        "        output *= self.s\n",
        "\n",
        "        return output, norm\n"
      ],
      "metadata": {
        "id": "vEhfHoOElyaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "num_classes = len(dataset.classes)\n",
        "\n",
        "backbone = models.resnet18(pretrained=True)\n",
        "backbone.fc = nn.Identity()\n",
        "backbone = backbone.to(device)\n",
        "\n",
        "magface = MagFace(\n",
        "    in_features=512,\n",
        "    out_features=num_classes\n",
        ").to(device)\n",
        "ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    list(backbone.parameters()) + list(magface.parameters()),\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-4\n",
        ")\n"
      ],
      "metadata": {
        "id": "0_6dsLhBR9CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    backbone.train()\n",
        "    magface.train()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        embeddings = backbone(images)\n",
        "        logits, norms = magface(embeddings, labels)\n",
        "\n",
        "        loss_id = ce_loss(logits, labels)\n",
        "\n",
        "        # MagFace regularization (norm constraint)\n",
        "        loss_reg = torch.mean((norms - magface.l_a) ** 2)\n",
        "\n",
        "        loss = loss_id + 0.01 * loss_reg\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {total_loss:.3f}\")\n"
      ],
      "metadata": {
        "id": "n_KBVXQyRREZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset_fd = FaceDetectDataset(val_dataset, mtcnn)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "val_loader_fd = DataLoader(\n",
        "    val_dataset_fd,\n",
        "    batch_size=32,\n",
        "    shuffle=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "hp6C4eQ46Rtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = []\n",
        "labels = []\n",
        "\n",
        "backbone.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, lbls in val_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        feats = backbone(imgs)\n",
        "        embeddings.append(feats.cpu())\n",
        "        labels.append(lbls)\n",
        "\n",
        "import torch\n",
        "embeddings = torch.cat(embeddings).numpy()\n",
        "labels = torch.cat(labels).numpy()\n",
        "\n",
        "print(\"Embeddings shape:\", embeddings.shape)\n",
        "print(\"Labels shape:\", labels.shape)\n"
      ],
      "metadata": {
        "id": "dhFy0l_ySO19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# If embeddings is a torch tensor on GPU\n",
        "embeddings = embeddings.detach().cpu().numpy()\n",
        "\n",
        "# Now normalize\n",
        "embeddings = normalize(embeddings)\n"
      ],
      "metadata": {
        "id": "elWrB5tal9Jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(\"Val dataset size:\", len(val_dataset))\n",
        "\n",
        "val_labels = []\n",
        "for _, lbl in val_dataset:\n",
        "    val_labels.append(lbl)\n",
        "\n",
        "label_counts = Counter(val_labels)\n",
        "\n",
        "print(\"Number of identities in val:\", len(label_counts))\n",
        "print(\"Images per identity (val):\", label_counts)\n",
        "print(\"Identities with >=2 images:\",\n",
        "      sum(1 for v in label_counts.values() if v >= 2))\n"
      ],
      "metadata": {
        "id": "YePdYjRwtQ1Q",
        "outputId": "51861a21-855b-4f20-aaa0-78dd68160695",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val dataset size: 1459\n",
            "Number of identities in val: 60\n",
            "Images per identity (val): Counter({117: 25, 185: 25, 275: 25, 22: 25, 196: 25, 87: 25, 223: 25, 95: 25, 285: 25, 65: 25, 137: 25, 98: 25, 217: 25, 140: 25, 24: 25, 62: 25, 116: 25, 86: 25, 44: 25, 7: 25, 38: 25, 172: 25, 202: 25, 85: 25, 67: 25, 284: 25, 245: 25, 74: 25, 32: 25, 162: 25, 109: 25, 79: 25, 251: 25, 261: 25, 153: 25, 201: 25, 180: 25, 220: 25, 266: 25, 151: 25, 210: 25, 258: 25, 241: 25, 193: 25, 206: 25, 84: 25, 283: 25, 219: 25, 43: 25, 2: 25, 272: 25, 228: 25, 195: 25, 47: 25, 273: 23, 212: 21, 239: 18, 55: 16, 164: 16, 205: 15})\n",
            "Identities with >=2 images: 60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WmFjCTeZuM2T",
        "outputId": "89ff8eee-0319-4d74-dfed-7125b61d25de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape: (1441, 512)\n",
            "Labels shape: (1441,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "id_to_indices = defaultdict(list)\n",
        "for idx, lbl in enumerate(labels):\n",
        "    id_to_indices[lbl].append(idx)\n",
        "\n",
        "pairs = []\n",
        "pair_labels = []\n",
        "\n",
        "# --- Positive pairs (deterministic) ---\n",
        "for lbl, idxs in id_to_indices.items():\n",
        "    if len(idxs) >= 2:\n",
        "        for i in range(len(idxs) - 1):\n",
        "            pairs.append((idxs[i], idxs[i+1]))\n",
        "            pair_labels.append(1)\n",
        "\n",
        "# --- Negative pairs (deterministic, balanced) ---\n",
        "all_labels = sorted(id_to_indices.keys())\n",
        "\n",
        "neg_pairs = []\n",
        "for i in range(len(all_labels)):\n",
        "    for j in range(i+1, len(all_labels)):\n",
        "        neg_pairs.append((\n",
        "            id_to_indices[all_labels[i]][0],\n",
        "            id_to_indices[all_labels[j]][0]\n",
        "        ))\n",
        "\n",
        "# Balance positives and negatives\n",
        "neg_pairs = neg_pairs[:len(pairs)]\n",
        "\n",
        "pairs.extend(neg_pairs)\n",
        "pair_labels.extend([0] * len(neg_pairs))\n",
        "\n",
        "pairs = np.array(pairs)\n",
        "pair_labels = np.array(pair_labels)\n",
        "\n",
        "print(\"Total pairs:\", len(pairs))\n",
        "print(\"Label distribution:\", np.unique(pair_labels, return_counts=True))\n",
        "\n"
      ],
      "metadata": {
        "id": "KW0zZGI1v4YC",
        "outputId": "0a541ff2-80fd-4f6c-c533-45419559ed5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pairs: 4214\n",
            "Label distribution: (array([0, 1]), array([2107, 2107]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "scores = []\n",
        "\n",
        "for i1, i2 in pairs:\n",
        "    scores.append(\n",
        "        cosine_similarity(\n",
        "            embeddings[i1].reshape(1,-1),\n",
        "            embeddings[i2].reshape(1,-1)\n",
        "        )[0][0]\n",
        "    )\n",
        "\n",
        "scores = np.array(scores)\n",
        "print(\"Scores length:\", len(scores))\n"
      ],
      "metadata": {
        "id": "lGx79ZJ8Zg_7",
        "outputId": "6b7386b9-6dcf-4282-ebf3-8bb1e99d56e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores length: 4214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"pair_labels unique:\", np.unique(pair_labels, return_counts=True))\n",
        "print(\"scores stats:\")\n",
        "print(\"  min:\", np.min(scores))\n",
        "print(\"  max:\", np.max(scores))\n",
        "print(\"  mean:\", np.mean(scores))\n",
        "print(\"Any NaN in scores?\", np.isnan(scores).any())\n"
      ],
      "metadata": {
        "id": "c90Os2nZvb9X",
        "outputId": "c58c965d-54f8-4f66-d5a6-5d87f36523eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pair_labels unique: (array([0, 1]), array([2107, 2107]))\n",
            "scores stats:\n",
            "  min: 0.3294811\n",
            "  max: 0.9994273\n",
            "  mean: 0.65321547\n",
            "Any NaN in scores? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "#embeddings = normalize(embeddings)\n",
        "\n",
        "# scores = []\n",
        "# for i1, i2 in pairs:\n",
        "#     scores.append(\n",
        "#         cosine_similarity(\n",
        "#             embeddings[i1].reshape(1,-1),\n",
        "#             embeddings[i2].reshape(1,-1)\n",
        "#         )[0][0]\n",
        "#     )\n",
        "\n",
        "# scores = np.array(scores)\n",
        "# pair_labels = np.array(pair_labels)\n",
        "\n",
        "fpr, tpr, _ = roc_curve(pair_labels, scores)\n",
        "fnr = 1 - tpr\n",
        "eer = fpr[np.nanargmin(np.abs(fpr - fnr))]\n",
        "\n",
        "print(\"EER:\", eer)\n"
      ],
      "metadata": {
        "id": "7-Y1H1kAu-sW",
        "outputId": "31f77040-16c3-49a6-dc21-cff0f2772193",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EER: 0.21547223540579022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "import numpy as np\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(pair_labels, scores)\n",
        "fnr = 1 - tpr\n",
        "\n",
        "# EER\n",
        "eer_idx = np.nanargmin(np.abs(fpr - fnr))\n",
        "eer = fpr[eer_idx]\n",
        "\n",
        "# FRR at FAR = 1%\n",
        "target_far = 0.05\n",
        "far_idx = np.argmin(np.abs(fpr - target_far))\n",
        "frr_at_far = fnr[far_idx]\n",
        "\n",
        "print(\"EER:\", eer)\n",
        "print(\"FRR @ FAR=1%:\", frr_at_far)\n"
      ],
      "metadata": {
        "id": "JfP-ZvYuepbr",
        "outputId": "600e9695-f3ba-47a0-b666-0f12942598f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EER: 0.21547223540579022\n",
            "FRR @ FAR=1%: 0.6416706217370669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/face_proj\n"
      ],
      "metadata": {
        "id": "TtKH-KI0k4gb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}